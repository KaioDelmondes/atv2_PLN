O objetivo deste trabalho é compreender a inserção de mulheres na Ciência da Computação. Buscou-se conhecer as percepções de professoras e professores a respeito da inserção das mulheres nessa área e as estratégias acionadas por elas para afirmarem-se nesse ambiente androcêntrico. Metodologicamente, realizou-se um estudo exploratório de natureza qualitativa com a utilização de entrevistas em profundidade com 12 docentes da área. Consideraram-se as percepções de ambos os sexos sobre o fenômeno, pois, como afirma Michelle Perrot,1 não se pode pensar na história das mulheres sem considerar os homens. Os resultados indicam que há formas sutis de discriminação e segregação das mulheres na área, exigindo delas esforço adicional para terem o mesmo reconhecimento que os homens.
Divulgo aqui alguns patches, principalmente o meu trabalho didático em Pure Data e Computação Musical. Pure Data (ou apenas “Pd”). Pd é um ambiente de programação usado para projetos interativos de computação musical (principalmente eletrônica ao vivo) e multimídia.
O objetivo deste trabalho é analisar questões relativas à profissionalização e profissionalidade docente no que se refere à Licenciatura em Computação. Neste estudo analisaram-se as matrizes curriculares de dois cursos de Licenciatura em Computação da UNEMAT, a legislação concernente, o contexto em que se inserem esses cursos e as discussões acerca do profissional professor de Computação. As normas que orientam essa formação e as propostas curriculares apresentam contradições: o curso é uma licenciatura, mas a definição do perfil profissional não se restringe à docência; os egressos serão professores da Educação Básica, mas não há esse campo de atuação ou disciplina previsto nos parâmetros que orientam a organização do ensino, exceto no Ensino Médio, em cujos PCNs são previstos conhecimentos, competências e habilidades na área de Informática. Tanto as Diretrizes para Formação de Professores quanto a proposta curricular da Sociedade Brasileira de Computação analisadas mostram-se orientadas pela demanda da sociedade contemporânea de profissionais “flexíveis”, aptos a atuar em mais de um campo de trabalho, dificultando a constituição da profissionalidade docente e o papel dos cursos de formação inicial em sua tarefa de contribuir para a profissionalização da profissão professor. Directory of Open Access Journals (DOAJ)
O objetivo deste trabalho é compreender a inserção de mulheres na Ciência da Computação. Buscou-se conhecer as percepções de professoras e professores a respeito da inserção das mulheres nessa área e as estratégias acionadas por elas para afirmarem-se nesse ambiente androcêntrico. Metodologicamente, realizou-se um estudo exploratório de natureza qualitativa com a utilização de entrevistas em profundidade com 12 docentes da área. Consideraram-se as percepções de ambos os sexos sobre o fenômeno, pois, como afirma Michelle Perrot, não se pode pensar na história das mulheres sem considerar os homens. Os resultados indicam que há formas sutis de discriminação e segregação das mulheres na área, exigindo delas esforço adicional para terem o mesmo reconhecimento que os homens.
Este trabalho aborda a dinâmica dos leilões de centavos (all-pay auctions), onde não só o vencedor do leilão paga pelo produto, mas sim todos os participantes gastam por haver necessidade de gastar dinheiro para exercer o direito de dar cada lance. É possível arrematar um produto por preço bem inferior ao seu valor nominal, mas também é possível gastar dinheiro e não levar absolutamente nada. Este fato levanta discussões acerca de se isso é ou não um jogo de azar, se há presença de estratégias para maximizar ganhos neste jogo e se há como o leiloeiro manipular esse jogo para maximizar os lucros de forma não legítima. Para estudar esses problemas foram usadas abordagens teórica e empírica, validando-se as hipóteses em bases de dados reais, discutindo-se os acertos e erros de cada abordagem.
A Internet vem ganhando notável importância nos dias de hoje, estabelecendo novas formas de comunicação entre diversos tipos de usuários e se firmando como relevante fonte de informações. Nesse contexto, a Wikipédia surgiu, introduzindo uma base de dados digitais online de caráter enciclopédico, para então se consagrar como uma das fontes de informações mais visitadas da rede. Seu diferencial em relação às demais enciclopédias está em seu processo editorial. Em vez de adotar um corpo de especialistas responsável pela edição de cada artigo, baseia-se no conceito de edição colaborativa, em que quaisquer usuários podem contribuir para a redação de páginas. Apesar de inovador e bem sucedido, esse modelo levanta questões importante, especialmente quanto ao grau de confiabilidade dos artigos: muitos contestam a utilização da Wikipédia como referência, principalmente em trabalhos acadêmicos. No presente trabalho foi realizada uma avaliação da credibilidade dessa ferramenta, a partir de dados de pesquisas e seguindo critérios metodológicos. Evidências como resultados satisfatórios em pesquisas, controle de qualidade efetivo pela comunidade e diferente apelo visual da Wikipédia garantem um nível adequado de confiabilidade para várias finalidades. Nesse sentido, ainda são levantadas ressalvas quanto ao tema abordado pelo artigo e o caráter de sua edição. Para aumentar ainda mais a credibilidade junto ao público em geral, neste trabalho propõe-se a criação de um "selo de validação de conteúdo", conferido por corpos editoriais de congressos e revistas especializadas, para atribuir valor quanto à validade do conteúdo de um dado artigo.
Este trabalho tem por objetivo estudar a aplicação da Internet e de seus vários recursos como ferramenta de elaboração e implementação de planos inovadores de Marketing Social por parte das instituições públicas e privadas, independentemente do seu tamanho ou área de atuação. A partir da abordagem moderna de conceitos como Marketing Social, Responsabilidade Social Corporativa e e-Business, aliada à análise de fenômenos historicamente recentes como o crescimento econômico e político das atividades do Terceiro Setor e o advento de tecnologias avançadas de comunicação social via Internet, será possível entender o processo de reformulação da visão estratégica de preocupação social das organizações com as comunidades em que estão inseridas. Tal entendimento visa tão-somente à compreensão do funcionamento e das vantagens de um novo modelo eletrônico de prática de Marketing Social pelas iniciativas pública e privada, proposto por este trabalho e de certa forma já realizado com relativo êxito por elas em muitos casos, alguns deles verificados neste documento.
Este artigo objetiva descrever a investigação desenvolvida no projeto Perspectivas teóricas sobre el património material e inmaterial en Sudamerica (Brasil y Argentina), que enfocou a constituição de patrimônios culturais através do órgão de gestão do patrimônio da cidade autônoma de Buenos Aires, a Comisión
 para La Preservación del Patrimonio Histórico de La Ciudad de Buenos Aires(C.P.P.H.C) e suas atividades ligada ao Sitio de Interés Cultural.
 Este ensaio visual apresenta fontes documentais utilizadas no desenvolvimento do trabalho de dissertação “A tradição das marcas de gado nos Campos Neutrais, RS/Brasil”. Esses documentos fazem parte de acervos existentes em departamentos de registros de marcas e sinais, arquivos
 históricos e bibliotecas no Rio Grande do Sul, Uruguai e Argentina. A partir destes documentos é possível vislumbrar as diferentes formas de registro e sistematização das marcas de gado na região, reconhecendo também estas fontes como um importante acervo a ser preservado.
 O objetivo do presente trabalho foi fazer uma análise geral da intimidade e da vida privada nos contextos da Teoria Geral do Direito e da sociedade da informação, utilizando isso para alertar ao engenheiro de Computação da necessidade de cientificar-se inteiramente disso e ressaltar a relevância do seu papel como instrumento valioso na busca de mecanismos necessários para o combate das práticas violadoras. O trabalho inicia conceituando e caracterizando a privacidade, apontando os titulares constitucionais dela. Seguindo, ele trata de aspectos mais abstratos do direito à privacidade e dos direitos fundamentais como um todo, tentando explicitar o caráter relativo e o âmbito de proteção daquele. Adiante, ele busca teorizar um pouco sobre os mecanismos de controle dos indivíduos pelo Estado, apresentando o panoptismo e seus conceitos. Daí, ele busca definir o escopo de sociedade da informação abordado aqui, conceituando-a e caracterizando-a. Por fim, abordam-se casos específicos e concretos de violação do direito à privacidade na sociedade da informação, tentando identificar os problemas em cada caso, apresentar as regulamentações nacionais e internacionais que ajudam na resolução deles e, eventualmente, sugerir alguma legislação ou postura ao ordenamento jurídico nacional. A idéia é fomentar a participação ativa dos engenheiros de Computação nesta área ainda em desenvolvimento.
 Este artigo aborda parte da pesquisa realizada no mestrado em Memória Social e Patrimônio Cultural da Universidade Federal de Pelotas, na qual abordou-se o produto do design gráfico como suportes memoriais de um tempo findo. Com este objetivo, através da oportunidade da realização de uma missão de estudos na cidade de Buenos Aires, buscou-se traçar uma perspectiva relacional entre o design gráfico produzido na cidade de Pelotas e na capital Portenha, no período de 1900 a
 1930, com ênfase no contexto no qual emergiram. A pesquisa na Argentina teve
 duração de dois meses, período no qual se procurou um estudo de caso semelhante ao que vinha sendo pesquisado no Brasil – peças gráficas associadas aos medicamentos do laboratório do Parque Souza Soares em Pelotas. Assim sendo, este trabalho investiga aspectos relacionados ao campo do design gráfico em espaços distintos de forma inserida ao universo da farmácia.
 Em vista de dar suporte às técnicas de comando e controle, este trabalho aborda o tema da análise semântica de frases escritas em linguagem natural. A partir dessa análise é possível melhorar a dinâmica de uso das ferramentas computacionais de suporte a decisão em linguagem BML - evitando que seus usuários precisem passar por longos períodos de treinamento. No contexto desse trabalho, o desenvolvimento que se pretende fazer é posterior à análise sintática, por isso, serão estudadas técnicas de inferência gramatical a partir das árvores sintáticas fornecidas pelo parser da língua portuguesa LX Parser.
 Este trabalho aborda a necessidade dos repositórios, especialmente os institucionais, construírem e divulgarem suas políticas de funcionamento. Analisa a ferramenta para construção de políticas de funcionamento de repositórios do OpenDOAR à luz das diretrizes propostas nas obras de Leite, Tomaél e Silva e Viana e Márdero Arellano. Constrói um instrumento para análise da ferramenta a partir da síntese das categorias encontradas nas diretrizes para construção de políticas propostas nas obras citadas. Compara as categorias de políticas da ferramenta com as da síntese das políticas que fazem parte do instrumento construído. Informa sobre os procedimentos e etapas que pautaram a pesquisa, um estudo de caso único. Reflete sobre o atendimento, por parte da ferramenta, das orientações propostas nas diretrizes. Conclui a respeito da contribuição que a ferramenta oferece aos responsáveis por repositórios na construção e divulgação das políticas de funcionamento dos mesmos. Deduz, amparado na literatura, sobre a promoção que a adoção e divulgação de políticas de funcionamento do repositório pode proporcionar ao mesmo. Propõe a realização de outras pesquisas a respeito da ferramenta, empregando outros métodos de análise.
 O uso de jogos com propósitos acadêmicos não é novidade, mas um tipo particular de jogos tem ganhado mais e mais força entre estudantes universitários recentemente: os Jogos de Negócio. Suas versões eletrônicas iterativas saíram de centenas para centenas de milhares de jogadores nos últimos 10 anos. Este trabalho de graduação implementa um ambiente para simulação de uma variedade de empresas que pode ser usado tanto como plataforma para criação e teste de jogos de negócios quanto como uma poderosa ferramenta de ensino e análise de cenários. A idéia principal é criar com código reutilizável um modelo mais profundo e mais flexível do que os gerados na bibliografia específica relacionada e trabalhos de graduação anteriores. A configuração apropriada do simulador resultante inclui a parametrização adequada dos departamentos de Suprimentos, Manufatura, Planejamento, Marketing, Vendas, Finanças e Recursos Humanos, além, obviamente, do mercado competitivo. Os resultados mostram que a versão final do simulador é capaz de não só gerar jogos de negócios coerentes com a teoria econômica e administrativa básica mas também de responder corretamente aos fenômenos de variação de câmbio, particularidades de exportação, estocasticidade do comportamento do mercado e inflação real.
 Este ensaio estuda as relações constituídas entre a Casa Museu João Luiz Pozzobon, seus visitantes e a comunidade local, em São João do Polêsine, interior do Rio Grande do Sul, Brasil. A Casa Museu é a réplica da casa do
 Diácono João Pozzobon que ficou conhecido por sua trajetória missionária por várias cidades do Estado, entre as décadas de 1950 e 1980. Através de depoimentos, do estudo sobre a memória de Pozzobon e do trabalho de
 observação em campo, a pesquisa objetivou identificar que relação se estabelece entre o espaço do museu e os visitantes que a ele afluem, bem como do museu com a comunidade local. Foi observada diferença de opiniões
 no que tange à santidade de João Pozzobon por parte dos visitantes da Casa Museu. Os visitantes oriundos de lugares distantes o veneram como um santo, os visitantes autóctones e comunidade veem João Pozzobon como um antigo vizinho, um homem comum, não considerando sua identidade de santo ou
 venerável.
 Este artigo objetiva descrever a investigação desenvolvida no projeto Perspectivas teóricas sobre el património material e inmaterial en Sudamerica (Brasil y Argentina), que enfocou a constituição de patrimônios culturais através do órgão de gestão do patrimônio da cidade autônoma de Buenos Aires, a Comisión
 para La Preservación del Patrimonio Histórico de La Ciudad de Buenos Aires(C.P.P.H.C) e suas atividades ligada ao Sitio de Interés Cultural.
 A decomposição de Cholesky é a fatoração de uma matriz hermitiana positiva definida no produto de uma matriz triangular inferior e a conjugada transposta desta, ou seja, uma matriz triangular superior. A aplicação do algoritmo de decomposição de Cholesky é vasta e sua eficiência é aproximadamente o dobro comparada ao algoritmo da decomposição LU. Por isso, a busca por métodos que aperfeiçoem tal algoritmo é de grande importância e impacto notável na performance de programas que utilizem essa decomposição. Este trabalho objetiva a discussão, implementação e análise de um novo modelo que busque obter uma melhor performance do algoritmo citado. Para tanto, é necessário fazer uso dos meios mais adequados de computação, comparar com os melhores modelos já existentes, e abordar a temática de um modo tratável. Logo, a utilização de processamento paralelo é primordial para atingir o objetivo e seu uso leva à comparação com o modelo já existente em Static Pipeline [citation]. Embora existam outros modelos, esse escolhido apresenta a melhor eficiência entre os estudados para a realização deste trabalho. O programa desenvolvido utiliza a ferramenta Charm++ [citation] como base da programação paralela e comunicação entre os núcleos de processamento. Apesar de tal programa possuir semelhanças com o código em Static Pipeline, a subdivisão de tarefas e a ordem de execução destas tornam o modelo diferente e visa alcançar uma maior exploração do paralelismo existente no algoritmo. Para analisar essa eficiência de paralelismo, mensurou-se tempos de execução, projetou-se traços de execução e construiu-se gráficos de performance, obtendo resultados favoráveis ao método estudado em um conjunto de casos experimentados.
 Este ensaio visual apresenta fontes documentais utilizadas no desenvolvimento do trabalho de dissertação “A tradição das marcas de gado nos Campos Neutrais, RS/Brasil”. Esses documentos fazem parte de acervos existentes em departamentos de registros de marcas e sinais, arquivos
 históricos e bibliotecas no Rio Grande do Sul, Uruguai e Argentina. A partir destes documentos é possível vislumbrar as diferentes formas de registro e sistematização das marcas de gado na região, reconhecendo também estas fontes como um importante acervo a ser preservado.
 Este artigo aborda parte da pesquisa realizada no mestrado em Memória Social e Patrimônio Cultural da Universidade Federal de Pelotas, na qual abordou-se o produto do design gráfico como suportes memoriais de um tempo findo. Com este objetivo, através da oportunidade da realização de uma missão de estudos na cidade de Buenos Aires, buscou-se traçar uma perspectiva relacional entre o design gráfico produzido na cidade de Pelotas e na capital Portenha, no período de 1900 a
 1930, com ênfase no contexto no qual emergiram. A pesquisa na Argentina teve
 duração de dois meses, período no qual se procurou um estudo de caso semelhante ao que vinha sendo pesquisado no Brasil – peças gráficas associadas aos medicamentos do laboratório do Parque Souza Soares em Pelotas. Assim sendo, este trabalho investiga aspectos relacionados ao campo do design gráfico em espaços distintos de forma inserida ao universo da farmácia.
 Com o avanço da tecnologia, da economia e do conhecimento da população, é claro, que o volume de transações financeiras que ocorrem por meio de cartão de crédito vai aumentar e para que esse crescimento seja sustentado, esse meio de pagamento deve ser cada vez mais seguro. O contexto de mais de 1 trilhão de reais transacionado em 2015 atrai fraudadores que querem dinheiro fácil e se sentem anônimos por estarem usando o cartão de outra pessoa. As fraudes são tão comuns que se pode dizer que os bancos as enxergam como custos operacionais, que são repassados para os consumidores e lojistas por meio de taxas em qualquer operação com o cartão. Esse trabalho, combina diversos algoritmos que buscam encontrar estabelecimentos fraudadores que processam transações com a Stone Pagamentos. O sistema implementado é baseado em regras feitas por especialistas de mercado e em casos passados. Esse modelo foi escolhido por ser fácil de modificar, de desenvolver e de construir. Além disso, é fácil de gerenciar a complexidade ou falta de informações e tem alto grau de precisão, facilidades de explicação e ter desempenho. Ainda podendo ser combinado com outras técnicas como redes neurais, algoritmos de regressão e arvores de decisão. Inicialmente, foram identificados 5 tipos principais de fraudadores e um desses tipos foi dividido em mais onze sub-tipos. Com o passar do tempo, novos tipos de fraudadores apareceram e outros algoritmos foram desenvolvidos, mas que não serão aqui apresentados por motivo de segurança para a empresa. Como esperado, esse sistema foi bem preciso e quando implantado em conjunto com especialistas do mercado a empresa teve seu primeiro mês sem prejuízo financeiro e esse período foi estendido por mais quatro meses.
 Este trabalho aborda a necessidade dos repositórios, especialmente os institucionais, construírem e divulgarem suas políticas de funcionamento. Analisa a ferramenta para construção de políticas de funcionamento de repositórios do OpenDOAR à luz das diretrizes propostas nas obras de Leite, Tomaél e Silva e Viana e Márdero Arellano. Constrói um instrumento para análise da ferramenta a partir da síntese das categorias encontradas nas diretrizes para construção de políticas propostas nas obras citadas. Compara as categorias de políticas da ferramenta com as da síntese das políticas que fazem parte do instrumento construído. Informa sobre os procedimentos e etapas que pautaram a pesquisa, um estudo de caso único. Reflete sobre o atendimento, por parte da ferramenta, das orientações propostas nas diretrizes. Conclui a respeito da contribuição que a ferramenta oferece aos responsáveis por repositórios na construção e divulgação das políticas de funcionamento dos mesmos. Deduz, amparado na literatura, sobre a promoção que a adoção e divulgação de políticas de funcionamento do repositório pode proporcionar ao mesmo. Propõe a realização de outras pesquisas a respeito da ferramenta, empregando outros métodos de análise.
 Este ensaio estuda as relações constituídas entre a Casa Museu João Luiz Pozzobon, seus visitantes e a comunidade local, em São João do Polêsine, interior do Rio Grande do Sul, Brasil. A Casa Museu é a réplica da casa do
 Diácono João Pozzobon que ficou conhecido por sua trajetória missionária por várias cidades do Estado, entre as décadas de 1950 e 1980. Através de depoimentos, do estudo sobre a memória de Pozzobon e do trabalho de
 observação em campo, a pesquisa objetivou identificar que relação se estabelece entre o espaço do museu e os visitantes que a ele afluem, bem como do museu com a comunidade local. Foi observada diferença de opiniões
 no que tange à santidade de João Pozzobon por parte dos visitantes da Casa Museu. Os visitantes oriundos de lugares distantes o veneram como um santo, os visitantes autóctones e comunidade veem João Pozzobon como um antigo vizinho, um homem comum, não considerando sua identidade de santo ou
 venerável.
 Este ensaio estuda as relações constituídas entre a Casa Museu João Luiz Pozzobon, seus visitantes e a comunidade local, em São João do Polêsine, interior do Rio Grande do Sul, Brasil. A Casa Museu é a réplica da casa do
 Diácono João Pozzobon que ficou conhecido por sua trajetória missionária por várias cidades do Estado, entre as décadas de 1950 e 1980. Através de depoimentos, do estudo sobre a memória de Pozzobon e do trabalho de
 observação em campo, a pesquisa objetivou identificar que relação se estabelece entre o espaço do museu e os visitantes que a ele afluem, bem como do museu com a comunidade local. Foi observada diferença de opiniões
 no que tange à santidade de João Pozzobon por parte dos visitantes da Casa Museu. Os visitantes oriundos de lugares distantes o veneram como um santo, os visitantes autóctones e comunidade veem João Pozzobon como um antigo vizinho, um homem comum, não considerando sua identidade de santo ou
 venerável.
 Redes sem fio vem se mostrando cada vez mais competitivas quando comparadas a redes cabeadas. Nesse contexto, um problema de difícil solução e de grande importância é realizar um posicionamento eficiente dos roteadores. Neste trabalho é desenvolvido um algoritmo genético que leva em consideração um modelo simplificado de obstáculos, para posicionar roteadores de modo a se obter cobertura total de um conjunto sensores fixos e total conectividade entre os roteadores. Os testes indicaram que o algoritmo obteve um resultado eficiente em comparação com soluções alternativas.
 Este ensaio apresenta uma breve discussão teórica sobre as perspectivas da pesquisa
 arqueológica na Charqueada Santa Rita (Pelotas/RS). Para isso desenvolve-se um diálogo dos referenciais teóricos e metodológicos da Arqueologia afro-americana e afro-brasileira com obras historiográficas que abordam o tema da escravidão em Pelotas. O intuito é apresentar novas questões para discutir a representação da memória e da identidade afro-brasileira em Pelotas.
 Os objetivos deste trabalho foram: estudar possibilidades de melhoria de desempenho e qualidade de algoritmos de reconstrução de Tomografia por Emissão de Pósitrons, através do estudo do algoritmo Siddon de cálculo do caminho radiológico e implementar um método de avaliação de imagens médicas utilizando a teoria de observadores numéricos. As possibilidades de melhoria estudadas foram o uso de simetrias entre projeções, dentro de uma projeção e dentro de uma linha de projeção e a avaliação correta do operador de Retro-projeção para que ele estime corretamente as probabilidades. Quanto ao uso de observadores numéricos, uma plataforma em Matlab foi implementada visando facilitar o processo de extração de Regiões de Interesse e cálculo de métricas das imagens.
 O presente trabalho tem como objetivo apresentar, como prova de conceito, a exeqüibilidade de um processo denominado matching de perfis de redes sociais, isto é, a capacidade de relacionar perfis de redes sociais distintas como uma maneira alternativa de extração de dados quando estes não são possíveis de se obterem de maneira direta. Utilizando uma base de 63 pessoas com perfis no Twitter e no Facebook, estudaram-se três processos contínuos de análise de dados para o matching: em um primeiro estágio, foram utilizados três algoritmos de matching de cadeias de caracteres, dentre eles o algoritmo de Levenshtein, o algoritmo de Knuth-Morris-Pratt e o algoritmo baseado em automata, com o intuito de estudar a relevância de atributos simples de perfis como nomes e identificadores; em um segundo estágio, foi realizada a comparação de atributos entre redes sociais distintas (Twitter e Facebook) para analisar a contribuição do cruzamento de dados ao resultado do primeiro estágio; no terceiro estágio, foi realizada a análise de vizinhança no grafo de relacionamentos de cada pessoa com o objetivo de estabelecer um critério de melhoria para o resultado do segundo estágio. O resultado dos três processos demonstrou que as comparações de atributos simples do primeiro estágio foram as mais relevantes para o processo de matching, contribuindo com mais de 60% de perfis com matching correto. O segundo estágio revelou-se importante para desambiguação de resultados de matching do primeiro estágio, aumentando para mais de 80% o número de casos de acertos. Em contrapartida, o terceiro estágio revelou-se irrelevante para o processo, aumentando em menos de 1% o resultado obtido no segundo estágio. Com o resultado significativo obtido nos dois primeiros estágios, fica demonstrado como prova de conceito a possibilidade de realizar matching de perfis de redes sociais.
 Um sistema de gerenciamento de nuvem 'e um tipo de software utilizado para gerenciar e orquestrar diversos recursos f'?sicos de modo a disponibiliza'-los para uso escala'vel por meio de m'aquinas virtuais, containeirs ou simplesmente o uso direto desses recursos. Tais sistemas possuem um componente denominado alocador, responsa'vel pela escolha de quais recursos sa˜o usados para uma determinada tarefa ou aloca¸ca˜o. Este trabalho propo˜e o uso de paraˆmetros da rede, como latˆencia, perda de pacotes e uso de banda, para a tomada de decis˜oes sobre quais os servidores mais apropriados para a aloca¸c˜ao de ma'quinas virtuais. 'E proposto tamb'em um m'etodo para colheita de dados de maneira a utilizar varia'veis estat'?sticas mais relevantes ao sistema. O m'etodo foi implementado no OpenStack, sistema mais popular de gerenciamento de nuvem.
 O Objetivo deste ensaio visual é apresentar um pouco da história de Santo Ângelo através das três igrejas construídas onde atualmente é o Centro Histórico do Município. Serão apresentadas a igreja da Redução Jesuítica de
 Santo Ângelo Custódio, a segunda que foi construída com os repovoadores e a terceira e ainda existente, a Catedral Angelopolitana. O uso de fotografias é uma maneira de ativar percepções atuais da história através do estímulo visual proporcionado pelas imagens. O trabalho foi elaborado com base em
 bibliografias sobre Santo Ângelo e em fotografias de arquivo pessoal e Arquivo
 Histórico Municipal.
 Com o aumento do poder computacional de processamento gráfico, se torna possível a criação de ambientes cada vez mais próximos do real. Em tais ambientes é necessário se ter definido claramente a ação física dos agentes no ambiente bem como seu comportamento. Neste trabalho, foi implementado um ambiente 3D, onde é possível ser testado facilmente pelo usuário algoritmos tanto de física quanto de inteligência artificial, sem que o usuário tenha conhecimento aprofundado sobre o funcionamento do programa. O ambiente desenvolvido tenta simular um ambiente real com carros voadores atuando como agentes sendo que um deles pode ser controlado pelo usuário.
 O ensino de Inteligência Artificial é consagrado nos cursos de Ciência e Engenharia da Computação. Dentre os vários métodos de ensino, existem os que usam competições para motivar o aprendizado e a participação dos alunos. O Mjollnir é uma plataforma web para competições de inteligência artificial com ferramentas que auxiliam e facilitam seu uso em sala de aula, especialmente por possibilitar a realização de torneios. O projeto inclui uma máquina virtual que auxilia os alunos a desenvolver e testar suas soluções no modelo necessário para ser usado na plataforma web, sem que o aluno precise se preocupar com a sincronização com outro jogador e disponibilizando quatro opções de linguagens de programação que podem ser usadas pelos alunos: C++, Java, C\# e Python. O projeto foi aplicado em duas turmas de inteligência artificial do ITA e uma pesquisa de opinião sobre o uso do Mjollnir foi respondida pelos alunos. Os resultados indicam que o incentivo a aprender IA depende do jogo utilizado, mas que, quando bem escolhido, ele faz com que os alunos se esforcem mais e pesquisem mais sobre algoritmos de IA. Quanto à plataforma, os formulários mostram que ela facilita o desenvolvimento dos agentes e fornece uma boa interface de depuração local, mas que ela também contém algumas falhas, como um alto tempo de turno, o que prejudica agentes com aprendizado por reforço, e como uma falta de logs para depuração online.
 Projeto e implementação de um sistema eletrônico para processamento de um sinal de áudio recebido de um instrumento musical a partir de vários captadores. Será feita mixagem de canais, já tratados separadamente, para posterior utilização do sinal resultante em um amplificador de potência externo, e também para posterior utilização do sinal resultante em outra etapa de amplificação suficiente para excitar alto-falantes de um fone de ouvido.
 Esse trabalho apresenta uma análise comparativa de resultados e implementação de otimizações para 3 algoritmos de detecção de estrutura de comunidades em redes complexas. Os métodos incluem abordagem por evolução diferencial e por otimização de 2 funções utilidade que partem de heurísticas distintas. A primeira delas fornece uma perspectiva local do nó em análise, se baseando apenas em informação de sua vizinhança para o cálculo da utilidade. A segunda se baseia na otimização em uma popular métrica para medição de qualidade da estrutura de comunidades de uma rede, o coeficiente de modularidade. Este trabalho inicialmente verifica os resultados citados na literatura, testa e identifica limitações dos métodos e apresenta otimizações utilizando técnicas de programação dinâmica e agregação de comunidades. Os testes são realizados sobre um conjunto de redes de pequena e média escala, indo desde uma rede social de 34 membros de um clube de karate até uma rede biológica de iterações entre proteínas de leveduras, com 2361 indivíduos e 10687 ligações.
 O artigo trata de uma análise breve da legislação recente da Argentina acerca do patrimônio cultural imaterial e sua relação com os instrumentos brasileiros. Trata-se de um estudo sobre as ligações entre as duas legislações e seus pontos de toque e suas diferenças. Será visto que as ações legislativas seguem um sentido comum embora sejam orientadas por atos de gestões do patrimônio cultural distintas.
 Empresas de software têm como principal custo o trabalho de sua mão de obra especializada e startups dispõem de recursos limitados para desenvolver um produto bem-sucedido. No mercado competitivo e dinâmico do desenvolvimento de software, as empresas que têm mais dificuldade de se adaptar são rapidamente ultrapassadas. Para uma nova empresa tentando se estabelecer, é imprescindível desperdiçar o mínimo de recursos. Enquanto as Metodologias Ágeis se concentram na eficiência e adaptabilidade operacionais, Lean Startup guia a estratégia de um novo empreendimento. Apesar de os métodos ágeis apresentarem potencial aumento de produtividade, cabe ressaltar que grande parte de seus benefícios se originam do elemento humano do desenvolvimento de software e da interação social entre os desenvolvedores. Mesmo assim, as práticas de níveis mais altos, como Scrum e Lean Startup, mostram impacto positivo sobre a velocidade de adaptação do produto, a qual permite providência de valor e cobrança de preços para sustentar a empresa de software.
 Este trabalho tem como objetivo estimar as empresas em melhor situação financeira, através da aplicação da Análise por Envoltória de Dados (DEA) aos dados contábeis de empresas de um mesmo setor. Desta forma, são determinadas as melhores opções de investimento dentre aquelas analisadas. A Análise por Envoltória de Dados (Data Envelopment Analysis - DEA), é capaz de calcular a eficiência relativa de uma dada entidade, quando comparada a outras do mesmo tipo, durante a utilização de determinadas entradas para a produção de determinadas saídas. As entradas e saídas utilizadas foram índices calculados a partir dos dados contidos nas demonstrações contábeis das empresas. Estes índices foram escolhidos para a otimização dos resultados na aplicação da DEA. Foi necessário um estudo criterioso para a realização de tais escolhas, pois esta é a parte subjetiva e que influencia diretamente nos resultados. A título de aplicação da metodologia proposta, foi feito um estudo de caso para o setor bancário, aplicando a DEA para quatro anos consecutivos, para oito bancos brasileiros. A aplicação da DEA por um determinado período de tempo foi um fator inovador deste trabalho, pois permite a análise da tendência da situação financeira ao longo de tempo, e não apenas a apresentação de um panorama estático obtido em outros trabalhos. Os resultados apresentaram alto grau de correlação com o valor da ação de cada banco na bolsa de valores, o que indica que este método pode ser bastante eficiente para a escolha das melhores opções de investimento, desde que os índices escolhidos para a aplicação da DEA sigam os critérios básicos descritos no presente trabalho.
 Este trabalho analisa a arquitetura ARM, a sua performance e o seu comportamento e explica os resultados através de informações retiradas da sua documentação oficial, com experimentos que comprovam a teoria. Os principais fatores abordados são: a performance no cálculo de stêncil, a performance de pico e os fatores que limitam a performance.
 Máquinas de busca são ferramentas poderosas usadas diariamente por milhões de pessoas no mundo. Dentro de sua estrutura o coletor de páginas é um passo importante para seu funcionamento correto e eficiente. O estudo de um coletor de páginas também é importante para análises da estrutura da web e serve como aprendizado em diversas áreas da Engenharia de Computação. Nesse trabalho analisa-se as dificuldades impostas pela internet ao programa, a arquitetura escolhida, técnicas alternativas e resultados obtidos do funcionamento do programa.
 O presente trabalho trata de um diagnóstico sobre o estado de conservação das obras raras da Biblioteca da Faculdade de Direito da Universidade Federal de Pelotas, datadas até 1840, onde mostra o que realmente são obras raras e se os cuidados para a conservação estão sendo adequados. Trata também de um apanhado histórico das obras raras no Brasil a diferenciação conceitual entre obra antiga e obra rara e sobre os critérios usados para a qualificação de obras raras utilizado pela Biblioteca Nacional, além da abordar a preservação e conservação de acervos bibliográficos, trazendo as diferenças conceituais entre preservação, conservação e restauração, e uma discussão dos resultados obtidos durante a realização do trabalho e da organização da sala de obras raras, resultados esses relacionados à idade do acervo e grau de conservação, entre outros. Abrange também a questão da preservação da memória e do
 patrimônio histórico-cultural.
 Este trabalho visa demonstrar o funcionamento básico de uma Maquina de Busca Web, descrevendo todos os seus componentes: o Coletor, que trata de colher e armazenar sites; o Indexador, que recolhe os textos úteis das paginas armazenadas e organiza seus dados em um índice; e o buscador propriamente dito, a parte mais transparente de uma maquina de busca, que processa uma consulta e organiza a lista de paginas que responde a tal consulta. Também almeja estimar a utilização dos recursos computacionais por cada componente do aplicativo, a fim de aperfeiçoar a sua utilização. Ainda se faz uma sugestão básica de implementação de uma Maquina de busca simples, com base em um computador pessoal, que serve como base para o estudo e para se analisar melhoras.
 Com o crescimento do setor aeroespacial, o domínio das tecnologias já existentes e o desenvolvimento de novas tecnologias mostram-se de vital importância para que os países possam ter autonomia nesse restrito mercado. Um dos principais projetos no setor aeroespacial em desenvolvimento no Brasil é a construção do foguete denominado Veículo Lançador de Satélites (VLS), o qual permitirá que o país tenha independência quanto à colocação de satélites em órbita. Os estágios finais do VLS têm por finalidade o posicionamento do satélite na órbita desejada, sendo que o controle de atitude do quarto estágio é realizado pelo sistema de basculamento. Neste trabalho busca-se a preparação, implementação e análise de desempenho de um sistema dedicado para o Simulador de Controle de Atitude do VLS, o qual realiza a simulação das manobras de basculamento. O sistema dedicado a ser implementado consiste na plataforma de controle NI CompactRIO, que é um avançado sistema embarcado reconfigurável de controle e aquisição.
 Este relatório apresenta a metodologia e os resultados finais obtidos a partir da
 atuação do Programa Regional de Educação Patrimonial – Memoriar nos municípios de Aceguá, Arroio Grande, Bagé, Capão do Leão, Cerrito e Jaguarão. Este programa se insere no convênio “Arqueologia e Educação Patrimonial na Região Sul do Rio Grande do Sul”, firmado entre o Laboratório de Ensino e
 Pesquisa em Antropologia e Arqueologia do Instituto de Ciências Humanas da Universidade Federal de Pelotas (LEPAARQ/ICH/UFPel) e a Votorantim Celulose e Papel (VCP) em cumprimento a legislação de proteção e valorização do patrimônio cultural.
 A utilizacao de ferramentas digitais para identificar redes sociais tem recebido atenção crescente de empresas em seus programas de marketing. Dentre programas desse tipo encontram-se os programas de indicação de produtos por clientes para seus contatos sociais. Neste trabalho de graduação, objetiva-se propor um sistema inteligente que melhore o desempenho de um programa de indicação para instituições de ensino superior, a partir do uso de técnicas de redes sociais e inteligência artificial. Os estudos feitos para criação de tal sistema incluíram a análise de um programa real de indicação aplicado em uma universidade. Os resultados obtidos indicam que houve melhoria no desempenho.
 Não existe muita uniformidade no cálculo do risco de instituições financeiras. Em comum as metodologias para estimação do risco requerem conhecimentos sobre a mecânica dos mercados de interesse, alguma sofisticação matemática, e sistemas computacionais e de informações confiáveis. No caso de risco operacional e risco legal o problema de medir risco deve ser tratado em uma abordagem caso por caso. No caso de risco de mercado e risco de crédito algumas metodologias já se encontram em uso, e explicadas na literatura de finanças. Neste trabalho nos concentramos em risco de mercado para efeito de exposição e comparação. E analisamos a aplicabilidade do CAPM MultiFator e do VaR.
 Foi realizado um estudo para identificar parâmetros de desempenho associados a indicadores de eficiência durante o parto de fêmeas suínas. Foram coletados os seguintes dados de 636 partos e 7.100 leitões nascidos: ordem de parto,
 número de leitões nascidos vivos, natimortos, mumificados, total de nascidos, aplicação de ocitocina, uso de palpação vaginal, momento do nascimento de cada leitão e do início e final do parto. O intervalo médio de nascimento dos leitões foi de 16,7 minutos e a duração média do parto, de 247 minutos. A duração média do parto aumentou conforme o total de leitões nascidos,
 o uso de ocitocina e a ocorrência de palpação vaginal. O total de nascimentos nos partos longos (> 211 min), de 13,1 leitões, foi maior que nos partos curtos (< 210 min), de 11,7 leitões, portanto, quanto maior o total de nascidos, maior a duração do parto. As leitegadas foram maiores nas fêmeas de segundo parto (12,8) em comparação àquelas de primeiro parto (11,9). A duração do parto é influenciada pelo total de leitões nascidos, que é maior em fêmeas de segundo parto e está associada à maior ocorrência de leitões natimortos e mumificados.
 O presente relatório visa descrever atividades desenvolvidas sobre a
 investigação e o acesso às fontes primárias de pesquisa na cidade de Buenos Aires – Argentina, realizada durante estágio acadêmico promovido pelo Mestrado em Memória Social e Patrimônio Cultural/ICH/UFPel, através do Programa de Cooperación Internacional Asociado para el Fortalecimiento de la Posgrado -Brasil/Argentina (CAFP/BA), no período de 01 de julho de 2009 a 6 de setembro de 2009. O trabalho de dissertação desenvolvido no Brasil estava inserido linha de pesquisa, Memória e Identidade e tinha como proposta de
 investigação a iconografia fotográfica nos periódicos ilustrados pelotenses,
 Jornal A Alvorada e Almanaque de Pelotas, durante o período de 1931 a 1935.
 Em geral, os modelos lineares com parâmetros constantes são aproximações ou idealizações de sistemas mais complexos. Com isto, projetos calcados nestes modelos lineares carecem de alguma especificação de até que ponto suas propriedades são garantidas como características do sistema real. A proposta deste trabalho é apresentar um estudo detalhado, usando a técnica de raio de estabilidade, de uma situação deste tipo onde a propriedade em foco é a estabilidade assintótica do equilíbrio. O estudo é focado em sistemas de segunda ordem variantes no tempo, com alguns casos não lineares, que simulam as incertezas estruturadas do sistema linear com coeficientes constantes. Para esses sistemas, são realizados estudos a cerca do raio real de estabilidade e do circulo limite. Uma análise sobre incertezas não estruturadas é feita usando a teoria de inclusões diferencias para sistema de segunda ordem.
 Este Trabalho de Graduação (TG) tem por objetivo fazer uma introdução a uma das principais técnicas de Inteligência Artificial, as Redes Neurais Artificiais (RNAs), aplicadas à previsão de Séries Temporais.No primeiro capítulo deste trabalho fez-se uma abordagem geral sobre Séries Temporais. A predição de valores futuros de uma série temporal é um problema extremamente comum em diversas áreas do conhecimento humano. Assim, podemos entender como modelos deste tipo de série são capazes de fornecer um auxílio para se fazer previsões e testar hipóteses a partir de dados existentes.No capítulo seguinte falou-se sobre as Redes Neurais Artificiais (RNAS), que são técnicas computacionais que apresentam um modelo inspirado na estrutura neural de organismos inteligentes e que adquirem conhecimento através da experiência. Assim, apresentou-se os conceitos gerais para, em seguida, dar um enfoque maior nas RNAs que utilizam o aprendizado supervisionado, como é o caso do algoritmo Backpropagation, pois estes tipos de redes é que são de maior interesse para o que foi desenvolvido neste trabalho. O terceiro capítulo é uma espécie de fusão entre o que foi visto no dois capítulos anteriores, pois é nele onde se encontra a construção de redes neurais para prognósticos de séries temporais, que é o objetivo principal deste Trabalho de Graduação. Nesta seção foi apresentada a metodologia proposta para a modelagem de arquiteturas de RNAs que seja capaz de realizar prognósticos confiáveis sobre valores futuros de séries temporais.Por fim, no último capítulo, encontram-se os resultados obtidos através das previsões realizadas com auxilio do software Forecaster XL, que é um "add - in" do Microsoft Excel. Este software é próprio para realizar previsões de séries temporais através de Redes Neurais Artificiais.
 Este artigo aborda os primeiros resultados de uma pesquisa sobre o Cine Theatro Grand Splendid realizada durante uma missão de estudos na cidade de Buenos Aires, Argentina. O objetivo da pesquisa foi estudar as ações
 patrimoniais ocorridas durante a mudança de função do antigo Cine Theatro Grand Splendid para livraria El Ateneo Grand Splendid. O Cine Theatro foi fundado em 1919 e permaneceu em funcionamento até o ano de 2000 quando
 fechou para abrir como livraria. O Grand Splendid representa, ao nosso entender, algumas mudanças significativas que ocorreram na indústria cinematográfica e no mercado editorial de Buenos Aires, durante a segunda
 metade da década de 1990. Neste artigo são apresentadas as fontes e locais de pesquisa, a história do Cine Theatro e a cronologia dos acontecimentos referentes à mudança de função e ao reconhecimento patrimonial.
 Este Trabalho de Graduação (TG) tem por objetivo fazer uma introdução a uma das principais técnicas de Inteligência Artificial, as Redes Neurais Artificiais (RNA), aplicadas à previsão de séries temporais. Foram abordados o histórico , principais conceitos, e sua forma de atuação, geral, na previsão de valores de uma série temporal, com enfoque no estudo de RNA's do tipo feedforward com algoritmo backpropagation. Foram realizadas algumas previsões do índice econômico ibovespa com auxílio do software Forecaster XL.
 Um grande desafio nas neurociências é a análise de correlação entre sinais de origem neuronal. Estes são adquiridos por diversos meios, incluindo o eletro-encefalograma (EEG) e implantes de vetores de micro-eletrodos. Foram estudados cinco métodos de detecção de correlação entre séries temporais, dois não-paramétricos: coerência espectral (SC) e coerência espectral parcial (PC); e três paramétricos: coerência parcial direcionada (PDC), função de transferência direcionada (DTF) e índice de causalidade de Granger (GCI). Estes últimos se baseiam na definição de causalidade de Granger para detecção de coerência direcionada, utilizando o modelo paramétrico de regressão linear. Os métodos foram testados quanto a sua eficiência e robustez na detecção de coerência, coerência direcionada e coerência parcial em diferentes conjuntos de sinais gerados artificialmente. Os sinais foram elaborados buscando reproduzir características de ruído e não-linearidade encontrados em dados neuronais reais. O método PDC mostrou-se o mais eficiente na detecção das diferentes características de correlação, enquanto os métodos não-paramétricos mostraram uma maior robustez a ruído e não-linearidades.
 Apresenta-se neste trabalho uma análise do desempenho das redes neurais artificiais para previsão de tendências das séries temporais de indicadores financeiros, a partir de uma comparação com o trabalho de graduação de Fábio Melo Pfeifer. São mostrados alguns princípios para operar no mercado financeiro, comparando alguns tipos de análises financeiras, além dos fundamentos de redes neurais artificiais e lógica fuzzy necessários para a compreensão do trabalho. São indicados os procedimentos para a construção de uma rede neural feedforward e de um sistema de inferência fuzzy no Matlab®. Com os resultados obtidos, foi feita uma comparação entre a rede construída no trabalho de graduação de Pfeifer e a rede desenvolvida no presente trabalho, através da comparação dos erros quadráticos médios apresentados pelas duas redes.
 Este trabalho apresenta estudos estatísticos sobre a performance do Instituto Tecnológico de Aeronáutica (ITA), no exame realizado pelo INEP o ENADE, em 2008. Para qualificar o desempenho do ITA, se utiliza dos resultados de outras instituições como forma de parâmetro comparativo, para tanto foram selecionadas as melhores colocadas, em termos de médias. Utiliza-se ainda do desempenho do ITA em 2005 como suporte para as análises efetuadas. Através de métodos estatísticos e probabilísticos busca comparar e retratar, de maneira detalhada e eficiente, os principais resultados acadêmicos, com a finalidade de medir a qualidade dos formandos. As comparações obtidas tentam aferir a diferença existente entre os alunos formados por esta instituição sobre os demais alunos das outras instituições usadas como parâmetro. Dado que os alunos do ITA recebem condições especiais na sua formação, tais como alojamento ou ainda alimentação, por exemplo, verifica se sob tais condições, os formandos de 2008 conseguiram obter um rendimento superior, como esperado.
 O objetivo deste trabalho é verificar experimentalmente se o fenômeno small-world está presente na rede de relacionamentos Orkut. O fenômeno small-world caracteriza-se por uma rede estruturada como um grafo que possui um baixo caminho médio característico e um alto coeficiente de agrupamento, isto é, a distância entre quaisquer dois nós da rede, em média, é pequena, enquanto a chance de existir uma aresta entre dois nós que possuem um vizinho em comum é, em média, alta. Os dados para análise correspondem a um subgrafo conexo, com 1.381.565 nós e 40.066.866 arestas, obtido a partir do grafo completo do Orkut e foram fornecidos pela empresa Google Inc., responsável pela rede. Para executar a análise, tratamentos estatísticos foram realizados sobre o subgrafo do Orkut e sobre modificações deste com diferentes topologias, com o intuito de estimar seus caminho médio característico e coeficiente de agrupamento. Os resultados indicam a presença do fenômeno no Orkut, porém não em sua intensidade máxima.
 O paradigma par-a-par (P2P) tem emergido como alternativa atraente ao tradicional modelo cliente-servidor para diversos sistemas distribuídos. A premissa fundamental de aplicações P2P é o compartilhamento voluntário de recursos por parte dos usuários participantes. Existe, entretanto, uma tensão intrínseca entre o interesse racional dos indivíduos e o bem-estar coletivo que limita o desempenho de tais aplicações. Este trabalho analisa o leecher problem do BitTorrent (BT), conhecido como free riding ou dilema social em outros contextos, sob duas perspectivas pouco exploradas anteriormente: possíveis alterações na topologia da rede BT sobreposta e questões lançadas pelo comportamento dos usuários finais. Fazendo uso de entrevistas, pesquisas e análises preliminares, conclui-se que significativos avanços na resolução do problema podem ser alcançados se a interface usuário-rede for aperfeiçoada, principalmente em facilidade de uso e com incentivos à cooperação.
 Este documento apresenta o trabalho de graduação do autor, que diz respeito a uma análise do sistema de controle do atuador do míssil MAA-1, desenvolvido pela empresa Mectron Engenharia, Indústria & Comércio S/A. Há um enfoque na especificação do controle digital do mesmo. Essa análise pode ser estendida para os outros mísseis desenvolvidos na referida empresa, uma vez que os sistemas de controle são muito semelhantes. Constam, inicialmente, informações a respeito do histórico da empresa, além dos setores da indústria de tecnologia de ponta em que atua nos dias de hoje. Em seguida, há uma breve explicação sobre o míssil MAA-1. Os tópicos que sucedem os referidos anteriormente, que dizem respeito ao míssil em questão, são: atuador; sistema de controle; controle digital; especificação de hardware para o controle digital; automatização de geração de código.
 O problema do caixeiro viajante é uma das mais clássicas formulações matemáticas no campo da Ciência da Computação. As possibilidades de abstração que tal problema apresenta são diversas, tendo diversas aplicações dentro da engenharia de transportes e da robótica. Por ser um problema NP-completo, o estudo de heurísticas e métodos para a resolução de instâncias desse problema são essenciais para se obter soluções cada vez mais satisfatórias. A análise da eficácia da aplicação de um algoritmo genético em parte da resolução de tal problema será tida como o principal objetivo deste trabalho, assim como evidenciar um conjunto de parâmetros do algoritmo que fornece, na média, as melhores soluções. Foram utilizadas duas estratégias para o algoritmo genético, que diferenciam-se apenas na definição da função de fitness. Em uma delas, a função de fitness levará em conta o custo total para visitar as cidades com um único viajante, para que ao fim do algoritmo, seja aplicado um algoritmo guloso que gera a solução final (a partir da solução simples) para múltiplos viajantes. Na outra, a função utilizará esse mesmo algoritmo guloso para cada cálculo do valor de fitness, levando em conta, portanto, o custo das viagens dos múltiplos viajantes. O benchmark utilizado foi obtido através de resultados simulados com o auxílio do otimizador CPLEX, um software proprietário desenvolvido pela IBM. Baseado em um modelo de programação linear inteira do problema, o otimizador pôde resolver várias instâncias do problema do múltiplo caixeiro viajante, permitindo uma boa verificação da qualidade das soluções obtidas com o auxílio do algoritmo genético. Os resultados são apresentados comparando as soluções obtidas com o auxílio do algoritmo genético com as obtidas pelo CPLEX e também com base em comparações com soluções do problema do caixeiro viajante simples fornecidos pela TSPLIB (uma biblioteca com várias instâncias do problema já resolvidas). São também providas sugestões para trabalhos futuros envolvendo a aplicação de diferentes tipos de algoritmos genéticos para a resolução do problema do múltiplo caixeiro viajante.
 Com o grande crescimento do número de redes de computadores e, conseqüentemente, da Internet, faz-se necessária, cada vez mais, a atualização das tecnologias utilizadas nestas aplicações. Por esse motivo, foi observado que o protocolo IPv4, amplamente utilizado, está ficando ultrapassado, e não mais atenderá as necessidades das redes de computadores, e da própria Internet, dentro de alguns anos. De forma a suprir estas necessidades, foi desenvolvido o protocolo IPv6, já encontrado em avançada fase de desenvolvimento e aplicação. Sua grande premissa é a de utilizar endereços maiores (128 bits, ao contrário do antigo, que utilizava apenas 32 bits), possibilitando um alcance de endereçamento muito maior, podendo incluir não apenas computadores, mas também dispositivos móveis e embarcados. Este novo protocolo também prevê uma série de melhorias em relação ao antigo, como implementação de cabeçalhos mais simples e organizados, além do uso de autenticação e criptografia já na camada de rede, e controle de fluxo e tráfego de pacotes, na tentativa de realizar uma melhor priorização de aplicações e qualidade de serviço. Este trabalho irá apresentar algumas características deste novo protocolo de comunicação, alguns aspectos em relação aos mecanismos de endereçamento e auto-configuração, e também alguns mecanismos de compatibilidade e transição do antigo protocolo IPv4 para o novo protocolo IPv6. Além disso, fornecerá à instituição uma fonte simplificada de referência para a transição inevitável de suas redes de computadores, facilitando assim o trabalho de pesquisa e aplicação dos responsáveis, e tornando a transição mais suave e acessível a todos os usuários de sua infra-estrutura, analisando tendências e apresentando alguns cenários de transição.
 A eletroencefalografia é uma das técnicas mais utilizadas para obter informações a respeito da atividade elétrica do cérebro de forma não invasiva. Devido à ligação com processos cognitivos, essa técnica pode ser utilizada para a construção de interfaces cérebro-máquina, onde um computador pode executar comando a partir da interpretação do eletroencefalograma (EEG) do usuário. Este trabalho estuda o processo de obtenção de sinais de EEG e suas características, descreve ferramentas de amplificação, interfaceamento com o computador e algoritmos de classificação. A seguir, experimentos com o ferramental descrito são realizados e seus resultados são discutidos, culminando na construção de uma prova de conceito de interface cérebro-máquina.
 O objetivo deste trabalho é realizar a análise e implementação computacional de três modelos para o problema do consenso em sistemas multi-agente. Problemas de consenso são problemas em que o objetivo é chegar a um acordo com relação a uma quantidade de interesse, que pode depender do estado de todas as entidades envolvidas na negociação. Os modelos estudados no presente trabalho são o Naming Game e dois outros nele inspirados. O Naming Game é um jogo baseado em linguagem e comunicação em que os agentes interagem para chegar ao consenso com relação ao nome de um objeto. Os demais modelos são variações do Naming Game, com mudanças na dinâmica de negociação entre agentes, representando a introdução de não determinismo na atualização das memórias locais dos agentes, no segundo modelo, e a introdução de memória compartilhada, no terceiro. Como os agentes utilizam somente de negociações locais entre pares e conseguem chegar ao consenso sem controle centralizado, os sistemas podem ser considerados sistemas auto-organizados. Os resultados mostram que, nos três modelos, o consenso pode de fato ser alcançado, e as condições para a ocorrência do consenso são apresentadas, juntamente com outras características que descrevem o comportamento dos modelos.
 O problema múltiplo do caixeiro viajante, embora não tão estudado quanto o problema do caixeiro viajante, é um problema bastante interessante e com diversas aplicações práticas. Apesar de possuir formulação simples, faz parte dos problemas NP-completos e, portanto, o uso de heurísticas é muito bem-vindo na busca de soluções aproximadas com tempo polinomial e que estejam a uma distância limitada da solução ótima. Os objetivos desse estudo são analisar a viabilidade de se implementar um algoritmo que use o método de Monte Carlo, cujo foco principal é um processo de decisão baseado em sorteios, bem como entender que tipo de ferramentas podem ser empregadas para melhorar métodos estocásticos no contexto do problema múltiplo do caixeiro viajante. Para fornecer embasamento, primeiramente, foi feito um levantamento rápido de heurísticas já empregadas, principalmente aquelas que se valem de algoritmos probabilísticos, a fim de propor um método que se utilizasse de um meio inovador para buscar a convergência para uma solução próxima à ótima. A fim de servir de base para a avaliação do método proposto que vale-se de reforço negativo, foi implementado um algoritmo guloso. Esse foi o único benchmark utilizado, uma vez que os resultados desse comparativo já foram suficientes para o julgamento da eficácia do algoritmo proposto. Foram feitos também estudos a respeito de um algoritmo estocástico que tem como ponto de partida o algoritmo guloso. São apresentados os resultados da comparação dos três algoritmos, observando as peculiaridades de cada um e comentando sobre a eficácia e desempenho das abordagens sugeridas. Além disso, são providas informações para auxiliar futuros estudos utilizando abordagens probabilísticas para o problema múltiplo do caixeiro viajante.
 O aprendizado de máquina e toda a sua funcionalidade tem revolucionado várias aplicações. Do processamento de linguagem à detecção de padrões em imagens. Dentro dessas diversas aplicações de machine learning, a predição do movimento de séries temporais é de suma importância e são aplicadas há inúmeros domínios. Exemplos de aplicações são previsão de temperatura, pressão e ventos em meteorologia; de expressão do gene durante o ciclo celular em biomedicina; e, por fim, a de índices financeiros e taxas de câmbio em finanças e economia. A aplicação de machine learning em finanças se configura como uma área vital para diversas empresas do ramo econômico, e é tratada como um desafio devido ao alto grau de imprevisibilidade. Este estudo busca implementar e estudar a viabilidade da aplicação do modelo support vector machine (SVM) na predição do movimento diário das ações, mais especificamente, na classificação sobre o movimento de subida ou descida das ações que compõem o índice Standard and Poor's 500. Os dados obtidos para o treinamento e teste dos modelos são obtidos através do yahoo finance. O treinamento do modelo support vector machine fez-se usando a função kernel gaussiana, tentando obter um que melhor resultado que se encaixe a não linearidade da natureza do problema. Por fim, é realizado uma comparação entre os resultados obtidos com o de outros estudos, buscando certificar a viabilidade ou não do support vector machine para a predição do movimento das ações e séries temporais.
 A interface cérebro máquina (ICM) é um caminho comunicativo entre o cérebro e um dispositivo externo, utilizando por exemplo padrões de eletroencefalografia. A eletroencefalografia (EEG) é um método de monitoramento muito utilizado para registrar a atividade elétrica do cérebro, normalmente não invasivo e de baixo custo. Este trabalho estuda os sinais de EEG e suas características, utilizando redes neurais de aprendizado profundo (deeplearning) para modelar abstrações de alto nível para os eventos coletados. O algoritmo elaborado será utilizado para controlar um braço robótico que realiza tarefas envolvendo movimentos com múltiplos graus de liberdade, culminando na prova de conceito de uma aplicação de interface cérebro máquina.
 Um Sistema de Informações Geográficas (SIG) integra hardware, software e dados para capturar, gerenciar, analisar e apresentar todos os tipos de informação geograficamente referenciada. Ele permite-nos ver, entender, questionar, interpretar e visualizar dados em várias formas as quais revelam relações, padrões, tendências na forma de mapas, globos, relatórios e gráficos. A quantidade de informações com as quais somos bombardeados aumenta a cada dia. Tomar decisões rápidas deixou de ser um diferencial e vem se tornando um aspecto essencial nas organizações. É bem sabido que ilustrações e figuras tornam um texto mais compreensível e torna ágil o entendimento das informações contidas em um documento. Dados dependentes de posição não precisam ser estudados na forma de lista, onde cada linha é um registro diferente. Ao invés, pode-se analisar visualizá-los em uma carta. Com um SIG pode-se fazer melhor uso de dados os quais levam a localização geográfica em consideração. Confrontando registros geográficos em um mapa, perceber detalhes pode ser mais facilmente verificado do que em uma tabela de banco de dados. Com o uso de SIG há inúmeras formas de manipular um banco de dados geográfico. Mapear a localização de algo facilita a tarefa de encontrá-lo. Tendo no banco de dados registros com a posição geográfica de determinada ocorrência, consegue-se ter uma visão do conjunto, observando todos os locais registrados de uma única vez. Em situações nas quais há registros demais para uma região pequena, uma solução é mapear a quantidade de registros verificados em uma determinada área. O uso de um cluster de ocorrências é observado na figura 1, na qual o balão com uma estrela representa a localização de dois registros próximos. Sendo insuficiente, mapea-se a densidade de ocorrências. Levando em consideração as utilidades geradas com o uso de um SIG, hoje este sistema é aplicado de formas diversas. Milhares de organizações usam SIG para resolver problemas e melhorar processos. Governos, empresas, ONGs e Forças Armadas já aplicam soluções baseadas em SIG. Exemplos de aplicações geográficas são: GeoBase, programa GIS desenvolvido pela United States Air Force (USAF); OPUS , sistema desenvolvido pelo Exército Brasileiro (EB) para controle de obras e ativos baseado em software livre; SPRING, sistema desenvolvido pelo Instituto Nacional de Pesquisas Espaciais (INPE); ArcGIS, desenvolvido pelo Environmental Systems Research Institute (ESRI).
 Uma rede complexa é um grafo com características topológicas não-triviais, características que aparecem comumente em modelagens de sistemas reais. Este trabalho apresenta um estudo em técnicas de análise de redes complexas e suas aplicações, particularmente em redes sociais. Para a análise, foram utilizadas duas redes de amigos do Facebook, das quais foi obtido o grafo completo através do uso das ferramentas Lost Circles e Gephi. Para melhorar a visualização dos grafos, foi aplicado o algoritmo Force Atlas, e para fazer o estudo de comunidades foi utilizado o algoritmo de classes de modularidade, e ao ser aplicado o método de Louvain pode-se vericar a segmentação correta dos indivíduos em comunidades. A partir desses grafos, e possível vislumbrar várias características e medidas da rede, e com isso foram calculadas as principais variáveis de rede (graus, betweenness centrality, agrupamento, modularidade) para verificar características small-world e scale-free. Com isso, pode-se concluir que ambas as redes possuem característica small-world, ou seja, há um baixo grau de separação entre os amigos de ambas as redes, porém apenas uma delas possui característica scale-free, ou seja, a distribuição exponencial de distribuição de grau não necessariamente segue os limites de scale-free.
 Esse trabalho de graduação se propõe a resolver alguns problemas interessantes na área de computação usando o ferramental matemático adequado, buscando sempre a forma mais simples de resolver cada problema.
 Desenvolvimento de um aplicativo com a finalidade de facilitar o manuseio e a visualização das informações necessárias ao desenvolvimento de um planejamento estratégico utilizando a metodologia do Balanced Scorecard (BSC). O trabalho incluirá um estudo de caso o qual envolve a aplicação de tal metodologia em uma Organização Não-Governamental, considerando as modificações necessárias devido às particularidades ligadas a esse modelo de organização.
 Este trabalho tem como objetivo estudar e aplicar uma estrutura para resolução de problemas de ciência de dados baseado no método científico. Para uma abordagem com caráter mais prático, escolhe-se um caso de estudo para exemplificar e explicar cada etapa do modelo. Utiliza-se uma base de dados aberta da ANAC, onde estão registrados dados de todos os vôos comerciais no período de 2000 à 2017, e nela é possível encontrar informações como o número de viagens, passageiros e capacidade total da aeronave. O problema de ciência de dados que se deseja solucionar a partir da base obtida é a predição do número de passageiros num avião, dados o aeródromo de origem e destino.
 Refatorações são processos de modificação de um sistema de software para melhoraria da estrutura interna do código sem alteraração de seu comportamento externo, permitindo melhoria contínua da programação, com o mínimo de introdução de erros e mantendo a compatibilidade com o código já existente. Refatoração em bancos de dados, por sua vez, é uma pequena alteração no esquema do banco de dados, cujo processo é usualmente caracterizado por duas fases, com definição de prazo limite para a conclusão da última fase a fim de possibilitar a adaptação por parte de responsáveis por aplicações que dependam do banco de dados refatorado em questão. A restrição da conclusão de uma refatoração apenas por prazo reduz a eficiência de tal processo, no caso dos responsáveis pelas aplicações dependentes desse banco de dados se adaptarem em data anterior ao prazo limite e a alteração demorar para entrar em vigor definitivamente, e também pode gerar incompatibilidades nas aplicações, quando estas não conseguirem se adaptar antes da expiração do prazo limite.A aplicação desse trabalho visa apresentar uma metodologia para gerenciar as refatorações de um banco de dados e a adaptação dos responsáveis pelas aplicações que dependem dessas alterações do banco, além de otimizar os processos de refatoração ao concluí-los assim que todos os responsáveis pelo desenvolvimento de aplicações dependentes desse banco atualizarem suas aplicações, e apenas quando os reponsáveis confirmarem a refatoração, impedindo a geração de incompatibilidades advindas de alterações no banco de dados de forma precipitada, isto é, antes que todas as dependências tenham se atualizado.
 Neste trabalho de graduação, aborda-se a teoria de matróides e como modelar problemas utilizando-a. Matróides são estruturas definidas sob um conjunto base de elementos e uma propriedade de independência, e, embora seja bastante aplicável na resolução de problemas, comumente não faz parte de cursos de graduação da área de computação. Este trabalho propõe-se a mostrar aplicações, assim como modelagem de problemas e sugestões de implementação para resolvê-los. Por último, o trabalho apresentará o problema de minimização de pilhas abertas, um problema de corte de padrões que não possui solução em tempo polinomial, e pretende-se analisar um de seus limitantes inferiores em tempo a partir da ótica de matróides, traduzindo o problema para o contexto de grafos.
 Geralmente, as aplicações utilizadas para realizar medições sobre determinados parâmetros demandam um conhecimento técnico para operar e interpretar os resultados. Essa correlação entre monitorar e administrar redes nos mostra a dificuldade de um usuário, sem o devido embasamento, observar o comportamento de sua rede local ao decorrer de um determinado intervalo de tempo. Logo, este trabalho consiste em permitir que o usuário final monitore sua própria rede local baseada na heurística de tradução dos dados coletados, tornando-os acessíveis a todos os níveis de conhecimento. Esta monitoração permitirá ao usuário verificar as chances de ter sucesso no uso de uma determinada aplicação ou na realização de uma determinada atividade online, além de permitir que ele acompanhe, ao longo do tempo, a qualidade da conexão em função de sua demanda.
 Os sites de Redes Sociais se tornaram um dos meios de comunicação e interação mais importantes do século XXI. Junto com a evolução da internet, com o maior acesso à mesma e com o aumento do poder de processamento dos navegadores e celulares, os sites de redes sociais passaram a estar presentes em nossas vidas a quase todo instante. Sendo assim, nesse estudo procura-se entender melhor as relações entre indivíduos em uma rede social e tentar quantificar o nível de interação desses indivíduos através da métrica desenvolvida.
 Este trabalho tem como objetivos apresentar metodologias para avaliação de cursos online de línguas, realizando um estudo de caso no curso Enlglish Talk. O trabalho se divide em duas partes, a primeira se destina ao desenvolvimento do curso, a segunda à apresentação da metodologia e sua aplicação. Na primeira parte deste trabalho, apresenta-se o English Talk, mostrando todo os elementos que compoem o curso, suas funções e significados. Há também um detalhamento importante na descrição de uma aula online e na descrição dos jogos utilizados no curso. Na segunda parte tem-se primeiramente a descrição dos métodos utilizados para avaliar os cursos online como todo o detalhadmento dos métodos. Posteriormente tem-se a descrição da aplicação de cada método ao English Talk e após os resultados obtidos na aplicação dos métodos.
 A virtualização de serviços de rede (NFV) é uma iniciativa de virtualizar serviços de rede hospedados em hardware dedicado. Desde sua primeira especificação oficial em 2012, diversas entidades como a Intel, Cisco e Nasdaq têm migrado para esse novo paradigma. Vantagens incluem flexibilidade, maior segurança e escalabilidade nas redes que implementam virtualização. Estudos mais recentes apontam para melhorias de performance em redes virtualizadas, incluindo maior throughput e menor latência. Apesar de muitos estudos sobre NFV terem sido realizados, não existe um consenso sobre qual a melhor plataforma ou implementação desse padrão. Neste trabalho foram analisadas algumas estratégias de virtualização de redes e implantadas em uma rede simulada. A plataforma utilizada foi o GENI, que fornece um testbed enorme de desenvolvimento em redes para diversas instituições aderentes. Por fim, analisou-se o comportamento da virtualização assim como a latência, throughput, packet loss e jitter da rede.
 Este trabalho promove o estudo de duas ferramentas comumente utilizadas no mercado que abordam a Gestão de Projetos: o MS Project 2000 e o Microsiga Protheus 7.01 - PMS. Foi estruturada uma metodologia de análise para o estudo das ferramentas segundo as orientações teóricas do guia PMBOK e a demanda por TI no tocante à Gestão de Projetos. O estudo busca agregar de forma crítica uma visão a abrangência necessária que os softwares devem possuir para satisfazer as necessidades básicas dos Gerentes de Projetos que praticam as melhores práticas do mercado na estruturação de seus projetos. Após a introdução dos principais pontos da Teoria do Gerenciamento de Projetos e dos conceitos de sistemas ERP (Enterprise Resource Planning), será apresentado um estudo detalhado da conformidade dos softwares com as melhores práticas do mercado e um estudo comparativo entre ambos. Será também, em anexo, apresentado um modelo de um Plano de Projetos de acordo com as recomendações do PMBOK.
 Esse trabalho de graduação consiste de um estudo sobre o framework OpenStack, conhecido como sistema operacional de nuvem, realizando a implantação em uma infraestrutura multi-servidor seguida de desenvolvimento de uma ferramenta de avaliação de desempenho através de benchmarking, proposta como solução para a grande complexidade de configuração de implantações desse sistema. OpenStack é um framework bastante poderoso e extremamente flexível, sendo composto de diversos módulos que podem ser instalados e configurados individualmente, responsáveis por tarefas específicas no data center, como gerenciamento de máquinas virtuais, controle da rede (possivelmente definida por software), fornecimento de armazenamento de dados como serviço, etc. Toda essa modularidade aumenta bastante a quantidade de configurações possíveis do sistema e portanto a dificuldade em se decidir que tipo de configuração utilizar.Uma ferramenta de benchmarking, portanto, auxilia nesse processo decisório por uma configuração de implantação, fornecendo algo mensurável entre diferentes configurações de modo que administradores do sistema possam tomar decisões mais orientadas no que adotar. O benchmark funciona a partir de um cenário padronizado, implementado sobre uma implantação funcional do OpenStack, do qual se retiram métricas relacionadas ao desempenho do sistema e que podem ser repetidas para diferentes implantações e comparadas entre si, para se decidir qual a mais apropriada para o sistema em questão.
 Este trabalho tem como objetivos construir um sistema que incentive a participação de respondentes em pesquisas de mercado conduzidas na internet através do uso da tecnologia Bitcoin. Para alcançar esse fim realizamos um estudo do mercado de pesquisas online e um estudo do funcionamento do Bitcoin, em seguida elaboramos um modelo de negócios que integre uma plataforma online com uma comunidade de pessoas criando assim um mercado livre de respondentes para pesquisa, os custos das transações financeiras são mitigados pelo uso de Bitcoin. Por último construimos um protótipo utilizando vários serviços online como base para testar como funcionaria essa plataforma. Foram conduzidos testes com usuários de todos os tipos para validar o modelo e a plataforma.
 O tema deste trabalho é descrever e implementar o algoritmo (de modo sequencial e em ambiente multicore) de Yanasse, Soma e Maculan ("An algorithm for determining the k-best solutions of the one-dimensional knapsack problem'', Pesquisa Operacional, vol. 20, n. 1, pp. 117-134, 2000), que encontra as K melhores soluções do Problema da Mochila Unidimensional (One - Dimensional Knapsack Problem), onde K ? ?. Foram utilizados alguns benchmarks para estudar o comportamento do tempo de execução do algoritmo, o que permitiu avaliar a eficiência da inserção do paralelismo por meio da linguagem "Open MP".
 A Retinopatia Diabética constitui-se como uma das mais importantes causas de perda visual em todo o mundo. Todavia, é possível reduzir as chances de cegueira por meio de um diagnóstico precoce, realizado com o auxílio de imagens de fundo de olho. A partir de um banco de dados fornecido pela California Health Foundation, em conjunto com a plataforma EyePACS, com cerca de 88 mil destas imagens rotuladas com o nível da doença, desenvolveu-se um algoritmo capaz de classificar o nível de Retinopatia Diabética de forma automática. Para tanto, técnicas de pré-processamento, como filtro Gaussiano e normalização de cores, foram empregadas para reduzir o ruído e a variabilidade do banco e, em seguida, utilizaram-se Redes Neurais Convolucionais como classificadores. Duas configurações de rede com diferentes profundidades e tamanhos iniciais de imagem foram avaliadas. As redes consistiram em primeiro plano de camadas de convolução e Max-Pooling, e, na parte final, foram combinadas camadas densas e de Maxout. Notou-se que ambas as arquiteturas alcançaram aprendizagem com convergência e resultaram em uma conformidade substancial com as classes esperadas, de acordo com a métrica Kappa Ponderado Quadrático. Ainda, observou-se que o aumento da profundidade da rede e do tamanho da imagem geram ganhos de qualidade, muito embora se incorra em queda de performance e aumento do consumo de memória durante o treinamento. Adicionalmente, verificou-se que a combinação de data augmentation, dropout e regularização L2 foram capazes de previnir um overfitting advindo da elevada complexidade da rede.
 Este Trabalho de Gradução tem como objetivo o uso de técnicas de Inteligência Artificial para criar uma estrutura de decisão capaz de aprender as decisões tomadas por um humano mediante um conjunto de exemplos e extrapolar tais decisões a casos novos que sejam apresentados. Ele é desenvolvido com o objetivo específico de agilizar o processo de classificação socio-econômica dos candidatos a aluno do CASD Vestibulares, ONG que promove ensino de qualidade e baixo custo a alunos carentes. Pretende-se que a estrutura de decisão desenvolvida seja capaz de substituir grande parcela do trabalho do examinador. A estrutura de decisão construída tem o formato de árvore de decisão cujos parâmetros foram determinados através de um algoritmo de aprendizado indutivo, denoninado INDUCTUVE TREE LEARNING, ou ID3. A seleção de atributos para a árvore é dada utilizando-se o cálculo da entropia do sistema a ser classificado em vários níveis distintos de organização. O resultado prático final deste trabalho é uma árvore capaz de decidir corretamente mais da metade dos casos com nível baixíssimo de falsos positivos, e portanto, de aplicação prática imediata.
 O projeto REALabs: Uma Federação de WebLabs Cooperativos, do programa Tidia/Kyatera, integrado pelas universidades UNICAMP, ITA, UFU, FACOM, PUC-RS e o Centro de Tecnologia da Informação Renato Archer - CTI, é um projeto que tem por finalidade criar um ambiente de laboratórios virtuais, usando a internet para que instituições tenham acesso a recursos de laboratórios de outras, de forma a possibilitar a realização de experiências remotamente. O ITA, representado pelo LMI - Laboratório de Máquinas Inteligentes, da Divisão de Eletrônica - Departamento de Sistemas e Controle, disponibilizará, inicialmente, seus dois robôs móveis - o ROMEO e Trekker, para que outras instituições possam explorar o ambiente e fazer a telemetria dos sensores dos robôs remotamente. Neste Trabalho de Graduação o robô móvel Trekker, da Superdroid Robots Inc., deve ser adaptado para ser comandado e monitorado à distância usando tecnologia Internet.
 Na escolha de um algoritmo criptográfico, algumas características são desejáveis e devem ser consideradas dependendo da utilização desejada. O presente trabalho coloca em foco o método criptográfico desenvolvido por Macedo (MACEDO, 2014), com o objetivo de compará-lo com outros métodos de criptografia simétrica já consagrados. O grande diferencial deste método se dá por conta da utilização de evolução de redes de autômatos, que é uma área que aparenta ter um potencial para o desenvolvimento de métodos criptográficos. São realizados experimentos de testes do tipo black box que analisam o desempenho dos algoritmos no quesito de confusão (o quão complexa é a relação entre criptograma e chave) utilizando conceitos de aleatoriedade. A aleatoriedade foi medida por uma medida de entropia explicada no capítulo 3, um conceito importante é que para cada tamanho de mensagem utilizado existe uma entropia de referência, que é a entropia alcançada em média por mensagens aleatórias daquele tamanho. Os resultados mostram que todos os algoritmos testados chegaram bem perto da entropia de referência. Quanto ao algoritmo de Macedo (MACEDO, 2014) os resultados dependem do número de passos de evolução utilizados, no caso foi utilizado N passos, onde N é o tamanho da mensagem. O número de passos utilizados se mostrou maior do que o necessário para se chegar à entropia de referência, também foi possível ver qual o número de passos necessários para se chegar próximo da referência. Essa comparação mostrou que os diferentes métodos têm resultados parecidos nos diferentes parâmetros testados e o método colocado em foco mostrou que existe potencial para criptografia com redes de autômatos.
 O trabalho consiste no desenvolvimento de softwares de comunicação de dados sem fio (wireless) os quais permitirão trocas de informações entre uma unidade remota e o simulador de Basculamento do VLS, possibilitando a passagem de dados de referência, atualização de parâmetros do controlador e ainda realização de funções de telemetria. O transceptor utilizado foi o Bluetooth via porta serial RS-232 devido a restrições do sistema operacional MS-DOS em acessar porta USB e o programa do computador de bordo foi implementado em linguagem C com biblioteca eRTOS por possuir funções necessárias para implementação de softwares embarcados em tempo real. O projeto tinha como requisito uma freqüência de execução de 100 Hz.
 Este trabalho tem como objetivo principal a construção de um aplicativo móvel usando uma ontologia de domínio e com consequência apresentar uma metodologia para a prototipação de aplicativos na área da saúde, usando para isso 2 ferramentas principais. O primeiro software é o PhoneGap que é responsável por transformar o código escrito em HTML, JS e CSS em uma linguagem nativa para o sistema operacional dos celulares, ou seja, com um mesmo código é possível criar aplicativos para iPhone, Android, Windows Phone e BlackBerry. O segundo software é o Protégé, que é de código aberto e é responsável por criar a ontologia que será utilizada para fornecer a base de conhecimento para o aplicativo. Com essas duas ferramentas uma metodologia para criar aplicativos foi criada para cada uma das plataformas dos celulares. Ademais, é possível realizar a manutenção dos protocolos de tratamento apenas mudando a ontologia e carregando uma nova, provendo versatilidade para o aplicativo.
 O projeto teve por finalidade a construção de uma ferramenta que possibilite que usuários gerem modelos baseados em voxels (volumetric pixels) em tempo real usando-se a ferramenta Unity3D como engine de apoio. O programa permite que o usuário possa criar meshes a partir de blocos elementares, denominados voxels. A geração dos vértices é feita por meio de um algoritmo muito conhecido na computação gráfica: Marching Cubes. A triangulação dos polígonos é tratada por um algoritmo conhecido como Triangulation by Ear Clipping. O trabalho visa mostrar melhor como foram inseridos os conceitos presentes nos principais algoritmos para a construção da Engine bem como a utilização dos voxels na geração de malhas procedurais.
 Este trabalho de graduação trata dos aspectos de segurança em rede, no tratamento dado para os incidentes de segurança em uma rede de computadores. O foco principal deste trabalho está centrado na rede interna da Aeronáutica, a INTRAER. Primeiramente foram analisadas as principais ameaças que uma rede de computadores está sujeita e seus principais atacantes em potencial, em seguida foi feito um levantamento da Política de Segurança dada ao sistema de Tecnologia da Informação na Aeronáutica e por fim realizada a proposta para a implantação de um Grupo de Resposta a Incidentes de Segurança em Computadores (CSIRT) para o COMAER no contexto de um Centro de Operação em Rede e Segurança (NOC/SOC). A rede interna de comunicação da Aeronáutica, a INTRAER, por ser bastante extensa e, principalmente, por ter um modelo em que cada OM (Organização Militar) é responsável pela sua rede local, sendo o DECEA o órgão responsável por prover o serviço de interconexão dessas redes através do TELESAT, ainda carece de uma estrutura adequada de segurança. Apesar dos esforços no sentido de melhor proteção dos dados e equipamentos de rede e também de uma política de segurança estabelecida em normas, não há garantia de segurança em diversos pontos das redes locais espalhadas por todo território nacional. Dessa forma, o CSIRT deverá fornecer informações técnicas e coordenar as ações em resposta a eventos que comprometam a segurança na rede, identificando as tendências nas atividades de ataques a sistemas computacionais, trabalhando em conjunto com outros especialistas em segurança para identificar soluções aplicáveis aos problemas de segurança e disseminar estas informações a toda a rede.
 O Brasil enfrenta hoje uma enorme carência de pessoas qualificadas para atuar nos segmentos tecnológicos. A área que atualmente mais cresce é a Computação Gráfica. Cada vez mais observamos os recursos gráficos elaborados em computador participarem de todos os ramos de atividades desde a indústria até o entretenimento. Hoje, nenhum componente mecânico é produzido sem antes ser criado virtualmente em um software de CAD, não há publicação impressa que não use um software de tratamento de imagem, nem filmes e programas de televisão que não utilizem ilhas de edição de áudio e vídeo. Estruturar um curso profissionalizante novo e de uma área de recente expansão é algo extremamente complexo e desafiante, pois não existe uma referência sobre qual é a melhor maneira de construí-lo, tão pouco o que é importante ser passado de conhecimento ao aluno. Para realizar esta árdua tarefa, foram levantadas as principais áreas de atuação do profissional deste ramo. A partir delas foram criadas disciplinas práticas que proviam os conhecimentos necessários para a atuação no mercado. Com essas matérias definidas, foi preciso criar outras que fornecessem os fundamentos teóricos e práticos para uma melhor compreensão e atuação nos segmentos que abrangem a Computação Gráfica. O curso foi então estruturado, relatando cada disciplina, sua ementa e a necessidade dela, respeitando as restrições do Ministério da Educação. Ele possui três módulos e a conclusão de cada um desses fornecesse ao aluno um certificado de Auxiliar, Assistente e Operador, respectivamente. Ao término do curso e após a conclusão do estágio curricular obrigatório é emitido o diploma que confere ao aluno o título de Técnico em Computação Gráfica. A aprovação do MEC foi conquistada em setembro deste ano, quando duas turmas, com 17 alunos cada, iniciaram o curso. Atualmente, o curso é ministrado na Alpha Channel Escola Profissionalizante que se situa na cidade de São Paulo - SP - Brasil. Os alunos aprovaram o curso e estão bem envolvidos com ele. Falta apenas uma resposta do mercado profissional, que, infelizmente, não pode ser observada até o término deste trabalho.
 Experimentos computacionais consistem na simulação de sistemas físicos através de códigos numéricos. Embora o poder computacional tenha crescido rapidamente, muitos problemas ainda são muito complexos, requerendo grande poder computacional, podendo tomar dias de processamento. Com isso torna-se interessante o uso de metamodelos para substituir as simulações, principalmente em fases inicias do projeto. Metamodelos são usados para aproximar fielmente a relação entre as variáveis de entrada e saída do sistema, sendo o custo computacional menor em comparação a simulações. Com isso, o objetivo final do trabalho é a construção de um pacote na linguagem R que auxilia o planejamento de experimentos computacionais (em inglês, Design and Analysis of Computer Experiments - DACE), focando na fase de análise.
 Esse trabalho tem como objetivo retratar o cenário atual de Big Data demonstrando através de quatro projetos a sua utilidade e os principais resultados delas. Além disso, são apresentadas o cenário atual de Big Data & Analytics no Brasil com uma perspectiva de expansão do mercado sinalizando as principais oportunidades e dificuldades das empresas adotarem essas novas ferramentas no dia a dia. Junto a isso, inclui dois Cases de sucesso do uso de Big Data, um no Starbuck que revolucionou os programas de fidelidades, facilitando demais a vida do consumidor e o recompensando-o toda vez que usa o aplicativo, e o outro é o case do Airbnb que vem criando ferramentas para o anfitrião de maneira que esteja mais munido de informação para que tenha mais sucesso de obter uma reserva. Iremos percorrer em cada um dos projetos, as principais motivações, a metodologia, o algoritmo em si, os resultados e algumas melhorias para futura versões. Descrevendo rapidamente cada um deles para ter uma ideia de o que teremos pela frente. Painel de Acompanhamento: Nesse capítulo serão apresentados dois painéis um para analisar a ruptura de produtos, que simplesmente é a falta de produtos na prateleira do supermercado, de maneira que a equipe de campo saiba atuar de uma maneira mais assertiva e rápida. A segunda parte mostrará o painel de execução de loja que antes era um desafio conseguir medir se uma loja estava boa ou não, e através de um fornecedor que trabalha com um reconhecimento de imagem, conseguimos tornar as fotos dos supermercados em número, e a partir daí fazer uma avaliação da loja. Algoritmo de Vendas: Já nesse capítulo, a ideia é desenvolver o algoritmo que já nos revele o problema e apresente a solução certa para cada um deles. No primeiro algoritmo, identifica as lojas com baixa performance e baseado em três problemas mais frequentes nesse canal, o algoritmo revela o mais crítico e mostra o produto a ser corrigido. No outro, enfrentamos um problema de desenhar o calendário de promoções mais assertivo que trará o maior retorno em vendas, porém dado a complexidade do mercado, não podemos mais replicar o mesmo para uma série de redes, porque dessa maneira somos pouco eficientes e com um custo altíssimo. Por fim, conseguimos concluir as principais oportunidades e dificuldades dessas ferramentas, e acredito que seja uma tarefa difícil de ser inserida no dia a dia da empresa, mas quem não conseguir fazer isso, pode acabar ficando para trás em relação a concorrência nos próximos anos.
 Nos últimos anos tem se assistido a um crescente processo de digitalização e automatização dos processos corporativos, movimento alavancado por fatores como barateamento de hardware, maior disponibilidade de conexão de rede e internet, assim como desenvolvimento das tecnologias de bancos de dados. Esse fenômeno tem aumentado muito a quantidade de dados sobre as operações das empresas que são gerados, processados e armazenados. Paralelamente a tudo isso, está havendo uma diminuição do ciclo de desenvolvimento dos produtos, devido a uma pressão dos consumidores por variedade e inovação. Diante de um quadro de abundância de dados, crescente acirramento da competitividade de todos os mercados de uma forma geral e grande exigência por parte dos consumidores, tem havido um aumento no interesse das corporações no uso de ferramentas analíticas como os Métodos Multivariados a fim de gerar vantagem competitiva. O objetivo deste trabalho é dar uma visão geral sobre Data Mining e Estatística Multivariada - que fazem parte do grupo dos Métodos Multivariados - no contexto do Processo de Desenvolvimento de Produtos. Para isso, o trabalho: Apresenta a teoria básica sobre Data Mining e alguns métodos da Estatística Multivariada, relacionando-a com o contexto do Processo de Desenvolvimento de Produtos; Utiliza o método estatístico multivariado MDPREF (Multidimensional Preference Analysis) num estudo de caso do mercado automobilístico brasileiro.
 Neste projeto se implementa um sistema para controle de automóvel autônomo baseado em visão em um ambiente simulado. Utiliza-se o framework de Deep Learning Caffe e o simulador de carros open source TORCS. Conta com a análise da utilização de diferentes arquiteturas de redes neurais convolucionais para extrair informações da imagem e utilizá-la para controlar um automóvel em um simulador e investiga a utilização de transferência de aprendizado nesse problema.
 Diferente do que muitas pessoas acreditam, a depress„o È uma doenÁa e n„o simplesmente um estado emocional passageiro caracterizado por tristeza. De fato, para que seja realizado o diagnÛstico correto de EpisÛdio Depressivo È necess·rio que o paciente apresente por pelo menos duas semanas humor deprimido e/ou perda de interesse ou prazer por quase todas as atividades na maior parte do tempo. Em crianÁas e adolescentes, o humor pode ser irrit·vel ao invÈs de triste. 
O presente estudo investigou as representaÁıes acerca da maternidade no contexto da depress„o pÛs-parto. Participaram do estudo duas m„es encaminhadas para a realizaÁ„o de uma psicoterapia breve pais-bebÍ devido ‡ presenÁa de depress„o no primeiro ano de vida de suas filhas. Entrevistas de avaliaÁ„o realizadas antes da psicoterapia foram analisadas a partir dos quatro eixos interpretativos que constituem a constelaÁ„o da maternidade, proposta por Stern (1997): vida-crescimento; relacionar-se prim·rio; matriz de apoio; e reorganizaÁ„o da identidade. Nos relatos de ambas as m„es apareceram representaÁıes acerca do sentimento de n„o ser capaz de cuidar do bebÍ logo apÛs o nascimento, de ser pouco apoiada pelo companheiro, bem como uma reavaliaÁ„o do relacionamento com suas prÛprias m„es e com seus cÙnjuges. Verificou-se tambÈm que as representaÁıes de cada m„e apontaram para uma estreita associaÁ„o entre seus conflitos pregressos e a interaÁ„o atual com o marido e com o bebÍ.
Este artigo apresenta algumas reflexıes sobre a produÁ„o de uma experiÍncia de subjetividade constituÌda a partir de saberes e pr·ticas que sustentam o enunciado da depress„o. Uma vez que pr·ticas e discursos sobre a normalidade e o desvio implicam em modos de subjetivaÁ„o, analisa-se aqui, sob a perspectiva foucaultiana e a partir de fragmentos de discursos presentes em fÛruns e enquetes de comunidades virtuais do Orkut, o trabalho de produÁ„o de si em referÍncia ao diagnÛstico da depress„o. Diante do imperativo de o indivÌduo saber sobre a verdade de sua sa˙de, cercado pelas exigÍncias de ideais normativos e imbuÌdo da crenÁa da necessidade de se proteger dos riscos ‡ sua vida, adotam-se diagnÛsticos como identidades, medidas preventivas e tratamentos medicamentosos como pr·ticas de si. 
Os diversos estados depressivos diferem em sua psicodin‚mica e nas narrativas que originam. H· um "discurso depressivo", onde o estado de ‚nimo È contextualizado, e os investimentos afetivos mantidos. Mas a capacidade de encadeamento de significantes depende do luto do objeto arcaico, luto esse impossibilitado na melancolia. Surge daÌ um "discurso melancÛlico" onde se observa uma ausÍncia de historicidade para os afetos, em uma narrativa niilista e pobre em expressıes fantasm·ticas. 
Este artigo discute a problem·tica da depress„o na adolescÍncia com base na vertente psicanalÌtica da Teoria do Apego. Primeiramente s„o estacadas as especificidades da adolescÍncia e sua relaÁ„o com o surgimento da depress„o. A partir daÌ, s„o apresentados os conceitos de funÁ„o reflexiva e de capacidade de mentalizaÁ„o, objetivando pensar o fenÙmeno depressivo na adolescÍncia como uma problem·tica dos vÌnculos afetivos. Conclui-se que h· uma associaÁ„o importante entre o estabelecimento de um padr„o de apego inseguro na inf‚ncia e o desenvolvimento da depress„o na adolescÍncia. A utilizaÁ„o dos conceitos de funÁ„o reflexiva e capacidade de mentalizaÁ„o permite reconhecer a import‚ncia da dimens„o representacional para essa problem·tica, proporcionando uma nova perspectiva para a compreens„o e abordagem terapÍutica da depress„o na adolescÍncia. 
Existe uma escassez de programas sociais e de sa˙de voltados para a manutenÁ„o do idoso dependente, o que leva ao aumento do n˙mero de idosos institucionalizados e a um quadro precoce de demÍncia e/ou depress„o. Objetivo: Avaliar os efeitos da intervenÁ„o motora com tarefa dupla na cogniÁ„o e presenÁa de depress„o nos idosos deste estudo e a aplicabilidade dos instrumentos Mini Exame do Estado Mental (MEEM) e AvaliaÁ„o Cognitiva Montreal (MoCA) na detecÁ„o de alteraÁıes cognitivas. Material e mÈtodos: Foi realizado um estudo experimental com 33 idosos: 14 idosos realizaram os exercÌcios com tarefa dupla por 5 semanas e foram chamados grupo intervenÁ„o (GI); 19 idosos n„o participaram da intervenÁ„o e foram chamados grupo controle (GC). Os grupos foram avaliados pelo: MEEM, MoCA e Escala de Depress„o Geri·trica (EDG). Resultados: N„o houve diferenÁa significante na funÁ„o cognitiva dos idosos apÛs a intervenÁ„o em nenhum dos dois grupos estudados. O GC apresentou na EDG escore acima de 5, indicando depress„o. Foi constatada uma forte correlaÁ„o entre a MoCA e o MEEM. Conclus„o: A intervenÁ„o fisioterapÍutica com tarefa dupla, durante 5 semanas, n„o foi suficiente para melhora cognitiva. O fato da presenÁa de depress„o no GC pode estar relacionada ao nÌvel de atividade e socializaÁ„o desse idoso. O MoCa mostrou-se um instrumento capaz de detectar comprometimento cognitivo leve, enquanto o MEEM foi um instrumento de mais f·cil aplicaÁ„o.
Os diversos estados depressivos diferem em sua psicodin‚mica e nas narrativas que originam. H· um "discurso depressivo", onde o estado de ‚nimo È contextualizado, e os investimentos afetivos mantidos. Mas a capacidade de encadeamento de significantes depende do luto do objeto arcaico, luto esse impossibilitado na melancolia. Surge daÌ um "discurso melancÛlico" onde se observa uma ausÍncia de historicidade para os afetos, em uma narrativa niilista e pobre em expressıes fantasm·ticas.
Existem evidÍncias da associaÁ„o entre asma e sintomas psiqui·tricos e transtornos mentais. Essa associaÁ„o pode resultar em dificuldades de se atingir o controle da asma. O objetivo deste estudo foi avaliar a associaÁ„o de ansiedade e depress„o com o controle da asma. M…TODOS: Estudo transversal com 78 pacientes asm·ticos com diagnÛstico confirmado de asma moderada a grave e regularmente tratados no AmbulatÛrio de Asma do Hospital S„o Paulo da Universidade Federal de S„o Paulo, S„o Paulo (SP). Os pacientes foram divididos em dois grupos em relaÁ„o ao status de controle de asma, determinado atravÈs do teste de controle da asma, e, posteriormente, comparados em termos de dados demogr·ficos, clÌnicos e espiromÈtricos, escore do question·rio de qualidade de vida para asma e escore da escala hospitalar de ansiedade e depress„o. RESULTADOS: A maioria era do sexo feminino. Dos 78 pacientes, 49 (63%) foram classificados como tendo asma n„o controlada. A prevalÍncia de ansiedade e do binÙmio ansiedade/depress„o foi significantemente maior entre os pacientes n„o controlados do que nos controlados (78% e 100%; p = 0,04 e p = 0,02, respectivamente), enquanto nem prevalÍncia de depress„o, nem os dados espiromÈtricos ou de qualidade de vida diferiram entre os grupos.
A depress„o apresenta alta carga de doenÁa no mundo. Fatores socioeconÙmicos e exposiÁ„o a situaÁıes extremas no trabalho podem estar associados ‡ doenÁa. O objetivo do trabalho È investigar a prevalÍncia e fatores associados ‡ depress„o em bombeiros de Belo Horizonte, Minas Gerais, Brasil. Estudo transversal foi realizado em universo de bombeiros do sexo masculino em Belo Horizonte (n = 711). O Invent·rio Beck para Depress„o (IBD) foi utilizado para avaliar a presenÁa de depress„o. Modelos de regress„o logÌstica (uni e multivariada) foram utilizados para estudar a associaÁ„o entre caracterÌsticas sociodemogr·ficas, estressores ocupacionais, situaÁ„o de sa˙de e depress„o. A prevalÍncia de depress„o na amostra estudada foi 5,5%. A chance de depress„o foi maior entre bombeiros que relataram sintomas de estresse pÛs-traum·tico (OR = 12,47; IC95%: 5,64-27,57) e uso abusivo de ·lcool (OR = 5,30; IC95%: 2,35-11,96). Os resultados s„o discutidos considerando as inter-relaÁıes entre transtornos mentais, o efeito do trabalhador sadio e o reconhecimento social do trabalho dos bombeiros.
Buscou-se identificar a prevalÍncia e os fatores associados ‡ ocorrÍncia de depress„o entre puÈrperas residentes em um municÌpio de mÈdio porte no extremo Sul do Brasil, durante todo o ano de 2013. Entrevistadoras treinadas aplicaram question·rio padronizado em todas as participantes nas duas ˙nicas maternidades do municÌpio. Investigaram-se caracterÌsticas demogr·ficas, socioeconÙmicas, comportamentais, de suporte social e morbidades. O rastreamento da depress„o foi realizado em atÈ 48 horas do puerpÈrio imediato, mediante a utilizaÁ„o da Escala de Edimburgo, sendo o ponto de corte = 10. Na an·lise multivariada, utilizou-se a regress„o de Poisson com vari‚ncia robusta. Das 2.687 mulheres entrevistadas, 14% (IC95%: 12,9-15,6) foram identificadas com depress„o. Fatores como depress„o anterior, tristeza no ˙ltimo trimestre da gravidez e historia de depress„o na famÌlia estiveram associados ‡ maior risco para depress„o, assim como ter menor idade e ser multÌpara. O suporte social fornecido ‡ gestante pela equipe de sa˙de foi um importante fator de proteÁ„o, reduzindo em atÈ 23% a raz„o de prevalÍncia de a puÈrpera desenvolver depress„o. Esses resultados indicam a necessidade de incrementar aÁıes por parte dos serviÁos de sa˙de em atenÁ„o ‡ gestante, a fim de prover-lhe maior cuidado nesse momento t„o delicado. 
Determinar os pontos de melhor sensibilidade e especificidade do Invent·rio de Depress„o de Beck (BDI) e da Escala de AvaliaÁ„o de Depress„o de Hamilton (HAM-D) no diagnÛstico de depress„o associada ‡ epilepsia. M…TODOS: Setenta e trÍs pacientes de um centro de referÍncia no tratamento da epilepsia foram submetidos ‡ avaliaÁ„o neuropsiqui·trica. Foram colhidos dados clÌnicos e sociodemogr·ficos, sendo utilizados os seguintes instrumentos: entrevista clÌnica estruturada (MINI-PLUS) para diagnÛstico psiqui·trico conforme o DSM-IV, HAM-D e BDI. RESULTADOS: No momento da entrevista, 27,4% dos pacientes estavam deprimidos e 37% preenchiam critÈrios para diagnÛstico de depress„o maior ao longo da vida. A an·lise da curva ROC indicou que o ponto de corte em 16 (> 16) para o BDI (sensibilidade de 94,4%, especificidade de 90,6%) e em 16 (> 16) para a HAM-D (sensibilidade de 95%, especificidade de 75,5%) representou dicotomizaÁ„o Ûtima entre deprimidos e n„o deprimidos. Ambos os instrumentos apresentaram um valor preditivo negativo superior a 95%. CONCLUS√O: A frequÍncia de depress„o maior È elevada em pacientes com epilepsia. BDI e a HAM-D podem auxiliar o clÌnico na identificaÁ„o da depress„o associada ‡ epilepsia, diminuindo seu subdiagnÛstico. 
A depress„o constitui a mais frequente perturbaÁ„o psiqui·trica entre indivÌduos infectados pelo HIV. Este trabalho pretende caracterizar a populaÁ„o de doentes HIV positivos da clÌnica de infectologia do Hospital de Joaquim Urbano do Porto quanto ao perfil de sintomas depressivos e verificar se estes se correlacionam com os par‚metros analÌticos mais frequentemente avaliados no contexto da infecÁ„o por este vÌrus: carga viral do HIV, contagem e percentagem de linfÛcitos CD4+. MÈtodos: Foi realizado um estudo observacional descritivo e analÌtico. Os nÌveis de sintomas depressivos dos participantes foram avaliados com o Invent·rio Depressivo de Beck. Os antecedentes patolÛgicos, psiqui·tricos e os valores analÌticos da carga viral, contagem e percentagem de CD4+ foram obtidos atravÈs de consulta aos respectivos processos clÌnicos. Resultados: Foi encontrada uma prevalÍncia de 65,5% de sintomas depressivos, com uma percentagem consider·vel (32,7%) dos indivÌduos apresentando sintomas graves. N„o se verificaram associaÁıes entre os nÌveis de sintomas depressivos e a contagem de CD4+, percentagem de CD4+ ou carga viral. Foram, no entanto, demonstradas associaÁıes entre sintomas depressivos, toxicodependÍncia e grau de escolaridade. Conclusıes: A elevada prevalÍncia de sintomas depressivos encontrada neste estudo reforÁa a import‚ncia da vigil‚ncia desse tipo de sintomatologia em indivÌduos HIV positivos. O fato de n„o se terem verificado associaÁıes entre sintomas depressivos e os par‚metros analÌticos avaliados est· em conformidade com estudos anteriores. 
A depress„o È um importante problema de sa˙de global e vem causando impacto negativo na vida dos indivÌduos e na de suas famÌlias, alÈm de elevar a demanda dos serviÁos de sa˙de. OBJETIVO: Verificar a prevalÍncia de depress„o e de fatores associados em indivÌduos com idade superior a 14 anos que buscaram atendimento na atenÁ„o prim·ria. M…TODOS: Estudo transversal em trÍs Unidades B·sicas de Sa˙de vinculadas ‡ Universidade CatÛlica de Pelotas. A depress„o, os transtornos de ansiedade e o risco de suicÌdio foram avaliados por meio da Mini International Neuropsychiatric Interview (MINI), enquanto os fatores associados, como idade, gÍnero, vive ou n„o com o/a companheiro/a, escolaridade e uso de subst‚ncias psicoativas, foram avaliados por meio de question·rio sociodemogr·fico. RESULTADOS: A prevalÍncia de depress„o foi de 23,9% (n = 256), apresentando-se mais evidente nas mulheres, com 4 a 7 anos de escolaridade, de classe socioeconÙmica D ou E, que abusam ou s„o dependentes de ·lcool, com algum transtorno de ansiedade e com risco de suicÌdio (p < 0,050). CONCLUS√O: Diante de tais resultados, salientamos a inserÁ„o de cuidados em sa˙de mental por meio de avaliaÁıes diagnÛsticas e de protocolos de atendimento que abarquem a depress„o e suas comorbidades na atenÁ„o prim·ria
Validar, adaptar e aferir a fidedignidade do Invent·rio de Depress„o Maior (Major Depression Inventory - MDI) para a lÌngua portuguesa. M…TODOS: O question·rio passou pelo processo de adaptaÁ„o transcultural. Foi realizado um prÈ-teste para avaliar sua aplicabilidade. Para avaliaÁ„o de reprodutibilidade, utilizou-se medida repetida com intervalo de 1 a 2 semanas e coeficiente de correlaÁ„o intraclasse. O MDI e a escala de Hamilton foram aplicados em 30 pacientes com diagnÛstico de depress„o, que foram pareados com 90 controles aos quais foi aplicado o MDI. A curva ROC foi realizada com 120 pacientes e escore final do MDI. Para an·lise da validade interna, utilizou-se o alfa de Cronbach. RESULTADOS: Sensibilidade e especificidade foram 0,86 e 0,75, respectivamente, com escores 16/17. O alfa de Cronbach para a escala total foi 0,91. O coeficiente de Pearson entre o total do MDI e o total da escala de depress„o de Hamilton foi 0,56. A an·lise fatorial revelou dois fatores: o primeiro explicava 53,9% da variaÁ„o enquanto o segundo explicava somente 13,6%. A confiabilidade teste-reteste foi excelente (com coeficiente de correlaÁ„o intraclasse variando de 0,50 e 0,93 para itens individuais e 0,90 para o escore total). CONCLUS√O: As propriedades psicomÈtricas do MDI se mostraram adequadas para aplicaÁ„o na populaÁ„o brasileira, entretanto outros estudos se fazem necess·rios.
Revisar a literatura sobre a associaÁ„o entre a amamentaÁ„o e a depress„o pÛs-parto. FontesUma revis„o da literatura encontrada na base de dados MEDLINE/Pub-Med. Resumo dos achadosA literatura mostra, de forma consistente, que a amamentaÁ„o fornece uma ampla quantidade de benefÌcios tanto para a crianÁa quanto para a m„e. Ainda s„o necess·rias mais pesquisas sobre os benefÌcios psicolÛgicos para a m„e. Alguns estudos apontam que a depress„o na gravidez È um dos fatores que pode contribuir para a n„o amamentaÁ„o. Outros estudos sugerem, tambÈm, uma associaÁ„o entre amamentaÁ„o e depress„o pÛs-parto, n„o estando clara ainda a direÁ„o dessa associaÁ„o. A amamentaÁ„o pode promover processos hormonais que protegem as m„es contra a depress„o pÛs-parto por atenuar a resposta do cortisol ao estresse. E isso tambÈm pode reduzir o seu risco, por auxiliar na regulaÁ„o dos padrıes do sono e vigÌlia da m„e e do filho, melhorando a autoefic·cia e o envolvimento emocional da m„e com a crianÁa, reduzindo as dificuldades de temperamento e promovendo uma melhor interaÁ„o entre eles. ConclusıesA pesquisa aponta que a amamentaÁ„o pode proteger as m„es da depress„o pÛs-parto e comeÁa a esclarecer que processos biolÛgicos e psicolÛgicos podem explicar essa proteÁ„o. Contudo, ainda existem resultados ambÌguos na literatura que poder„o ser explicados pelas limitaÁıes metodolÛgicas apresentadas por alguns estudos.
Este artigo analisa historicamente o background ideolÛgico que tornou possÌvel a transformaÁ„o da noÁ„o de melancolia nos conceitos de depress„o e transtorno bipolar, a partir das mudanÁas mÈdicas e psicolÛgicas ocorridas no decorrer do sÈculo XIX. A antiga noÁ„o de melancolia foi remodelada e sua transiÁ„o para a doenÁa depressiva foi facilitada pelo conceito de Iipemania de Esquirol, que, pela primeira vez, enfatizou a natureza afetiva prim·ria da doenÁa. Finalmente, uma vez obtidas as condiÁıes conceituais necess·rias, a melancolia e a mania foram combinadas no conceito de insanidade alternante, periÛdica, circular, ou de forma dupla, seus rÌgidos padrıes descritivos foram flexibilizados, tendo culminado este processo na sinopse de Kraepelin.
em-se observado que a depress„o È preditora de reinternaÁ„o e mortalidade na insuficiÍncia cardÌaca. O hormÙnio da paratireoide È um biomarcador novo e promissor que pode predizer a internaÁ„o, a capacidade funcional e a mortalidade na insuficiÍncia cardÌaca. OBJETIVO: Nosso objetivo foi investigar a associaÁ„o da depress„o aos nÌveis sÈricos de hormÙnio da paratireoide em pacientes com insuficiÍncia cardÌaca sistÛlica. M…TODOS: Cem pacientes ambulatoriais consecutivos com IC sistÛlica com fraÁ„o de ejeÁ„o do ventrÌculo esquerdo < 40% foram examinados prospectivamente. Todos os pacientes foram submetidos a exames laboratoriais, incluindo an·lises de peptÌdeo natriurÈtico cerebral e de hormÙnio da tireoide. Os pacientes foram convidados a completar o Invent·rio de Depress„o de Beck-II. RESULTADOS: Cinquenta e um pacientes (51%) apresentavam escore de BDI ruim (escore de BDI > 18). Esses pacientes apresentavam nÌveis de hormÙnio da paratireoide significativamente mais elevados em comparaÁ„o com aqueles com bons escores de BDI (133 ± 46 pg/ml versus 71 ± 26 pg/ml, p < 0,001). No modelo de regress„o logÌstica multivariada, constatou-se que o nÌvel do hormÙnio da tireoide (raz„o de chances (OR) = 1.035, p = 0,003), fraÁ„o de ejeÁ„o do ventrÌculo esquerdo (OR = 0,854, p = 0,004), classe funcional III / IV (OR = 28,022, p = 0,005), C-reactive protein (CRP) (OR = 1,088, p = 0,020) e presenÁa de edema prÈ-tibial (OR = 12,341, p = 0,033) constituÌam preditores independentes de depress„o moderada a importante apÛs o ajuste de outros possÌveis fatores de confus„o. CONCLUS√O: Pacientes com insuficiÍncia cardÌaca sistÛlica com depress„o moderada a importante apresentavam nÌveis sÈricos elevados de hormÙnio da tireoide e CRP, capacidade funcional ruim e fraÁ„o de ejeÁ„o do ventrÌculo esquerdo mais baixa. A associaÁ„o da depress„o com esses par‚metros pode explicar a contribuiÁ„o da depress„o para a internaÁ„o e a mortalidade na insuficiÍncia cardÌaca.
A depress„o È um problema de sa˙de p˙blica, em que cerca de 154 milhıes de pessoas s„o afetadas mundialmente, e os idosos enquadram-se neste contexto com um percentual de 15% de prevalÍncia para algum sintoma depressivo. O presente estudo teve como objetivo investigar como a literatura vem abordando as principais causas de depress„o nos idosos. MÈtodo:AtravÈs de uma revis„o sistem·tica da literatura, realizada nos meses de janeiro a abril de 2015, em que os dados foram organizados em categorias e analisados atravÈs da literatura pertinente. Resultado:Os resultados evidenciaram o principal perfil dos idosos acometidos pela depress„o, formas de tratamentos, chegada ao serviÁo de sa˙de e comorbidades associadas. A pesquisa tambÈm mostrou a import‚ncia do estÌmulo ao autocuidado, ativaÁ„o e engajamento das pacientes e familiares em atividades educativas, treinamento profissional de sa˙de e ampliaÁ„o do sistema como fatores importantes ao cuidado destes usu·rios. Conclus„o:Concluindo-se que os profissionais de sa˙de que lidam com essa faixa et·ria devem estar atentos aos sinais e sintomas da depress„o, alÈm de estarem constantemente se capacitando para atender as demandas de uma assistÍncia eficaz e de qualidade, estimulando tambÈm mÈtodos n„o farmacolÛgicos de alÌvio dos sintomas, sabendo identificar o perfil destes idosos e quais as comorbidades associadas a depress„o mais comuns. 
Comparar pacientes internados com depress„o e com transtorno de humor bipolar em episÛdio depressivo quanto aos nÌveis sÈricos de zinco. MÈtodos Foram incluÌdos 46 pacientes com idade igual ou superior a 19 anos, de ambos os sexos, internados em Unidade de InternaÁ„o Psiqui·trica de um hospital universit·rio do sul do Brasil. Os participantes foram divididos em dois grupos: Grupo Depress„o (Grupo D) e Grupo com Transtorno de Humor Bipolar em episÛdio depressivo (Grupo THB). A an·lise do zinco sÈrico foi realizada por meio de espectrofotometria de absorÁ„o atÙmica. Como valores de referÍncia para normalidade, foram adotados nÌveis de zinco sÈrico acima de 59 µg/dL para mulheres e acima de 61 µg/dL para homens. Resultados Os nÌveis de zinco sÈrico estavam dentro do padr„o de normalidade em 95,7% dos pacientes. A mediana de zinco no Grupo D foi de 88,5 µg/dL e de 81,5 µg/dL no Grupo THB, porÈm essa diferenÁa n„o foi estatisticamente significativa. O Grupo THB apresentou valores maiores de Ìndice de massa corporal (IMC), LDL colesterol e mais internaÁıes psiqui·tricas prÈvias. Conclus„o Os resultados encontrados no presente estudo mostram que os nÌveis de zinco sÈrico em pacientes em uso de antidepressivos e outras medicaÁıes psiqui·tricas, internados por THB em episÛdio depressivo, quando comparados a pacientes com depress„o, n„o diferiram e estavam dentro da faixa de normalidade. O uso dessas medicaÁıes pode ter influÍncia nas concentraÁıes sÈricas do mineral. 
Investigar se existe correlaÁ„o entre sintomas depressivos, sintomas ansiosos e psicopatia em 25 prisioneiros de um municÌpio do Rio Grande do Sul. MÈtodos Para a coleta de dados, foram utilizados os Invent·rios de Depress„o e Ansiedade Beck e a Escala Hare para psicopatia. As entrevistas foram realizadas nas dependÍncias de uma instituiÁ„o prisional de forma individual. Resultados Foi encontrada correlaÁ„o estatisticamente significativa entre depress„o e ansiedade, e o escore total de psicopatia n„o se correlacionou com ansiedade, somente com depress„o. Por outro lado, o fator 2 da escala, referente ao aspecto comportamental do transtorno, apresentou correlaÁ„o com ansiedade e depress„o. Conclus„o Embora alguns dados tenham sido concordantes com os da literatura, a pesquisa apresentou resultados n„o encontrados em estudos anteriores. Dessa forma, evidencia-se a necessidade de realizar novos estudos na ·rea. 
A depress„o È uma importante causa de suicÌdio em adolescentes. Portanto, s„o necess·rios instrumentos adequados para o rastreamento da depress„o nessa populaÁ„o. OBJETIVO: Avaliar escalas de depress„o como instrumentos de rastreamento para depress„o em estudantes brasileiros do ensino mÈdio. M…TODOS: Estudo transversal. TrÍs escalas (BDI, CES-D, e CRS) e um teste para avaliar sintomas psiqui·tricos gerais (SRQ) foram aplicados individualmente a 503 estudantes do ensino mÈdio com idades entre 15 e 17 anos. Os resultados foram comparados aos obtidos com os critÈrios de depress„o maior do manual diagnÛstico e estatÌstico de transtornos mentais (DSM-IV). RESULTADOS: A prevalÍncia de depress„o maior utilizando-se os critÈrios do DSM-IV foi de 10,9%. Adolescentes com depress„o maior apresentaram escores significativamente mais altos (p = 0,001) no SRQ e nas trÍs escalas avaliadas em comparaÁ„o ao grupo sem depress„o. A sensibilidade e a especificidade para identificar depress„o pelo BDI, CES-D e CRS foram, respectivamente, 0,77 e 0,70, 0,75 e 0,73 e 0,82 e 0,71 (curva ROC). Os melhores pontos de corte foram 9 para o BDI, 10 para a CRS e 14 para a CES-D. A frequÍncia de sintomas depressivos foi maior em meninas (aproximadamente 2:1)
A depress„o pÛs-parto È um transtorno de alta prevalÍncia que pode comprometer a qualidade da relaÁ„o m„e-crianÁa. Este estudo pretende determinar a prevalÍncia do referido transtorno, comparar a interaÁ„o m„e-bebÍ nos grupos com e sem depress„o e verificar a relaÁ„o entre depress„o, apoio social e estilos de relacionamento e disponibilidade emocional maternos. As participantes eram gestantes que pretendiam dar ‡ luz no Hospital Universit·rio da Universidade de S„o Paulo entre dezembro de 2006 e dezembro de 2008. A prevalÍncia de depress„o pÛs-parto em nossa amostra foi 28%. N„o houve diferenÁa significativa na relaÁ„o m„e-crianÁa no grupo com e sem depress„o. Encontrou-se correlaÁ„o positiva entre sensibilidade materna e escolaridade e entre sensibilidade e certas dimensıes de apoio social e estilo de relacionamento. Conclui-se que a prevalÍncia de depress„o pÛs-parto em nossa amostra È mais alta que a mÈdia mundial, mas a sintomatologia depressiva n„o interfere significativamente na qualidade da interaÁ„o m„e-bebÍ. A sensibilidade materna È influenciada por fatores sÛcio-cognitivos e afetivos
